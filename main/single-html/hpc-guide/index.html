<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLE-HPC 15 SP6 | Administration Guide</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Administration Guide | SLE-HPC 15 SP6"/>
<meta name="description" content="This guide covers system administration tasks such as …"/>
<meta name="product-name" content="SUSE Linux Enterprise High Performance Computing"/>
<meta name="product-number" content="15 SP6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tahlia.richardson@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise HPC 15 SP6"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Administration Guide | SLE-HPC 15 SP6"/>
<meta property="og:description" content="This guide covers system administration tasks such as remote administration, workload management, and monitoring."/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration Guide | SLE-HPC 15 SP6"/>
<meta name="twitter:description" content="This guide covers system administration tasks such as remote administration, workload management, and monitoring."/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-hpc/edit/main/xml/MAIN.hpc-guide.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-hpc">Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-hpc" data-id-title="Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Performance Computing</span> <span class="productnumber">15 SP6</span></div><div class="title-container"><h1 class="title">Administration Guide</h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/MAIN.hpc-guide.xml" title="Edit source document"> </a></div></div><div class="abstract"><p>
    This guide covers system administration tasks such as remote administration,
    workload management, and monitoring.
   </p></div><div class="date"><span class="imprint-label">Publication Date: </span>March 06, 2024
</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#preface-administration"><span class="title-name">Preface</span></a></span><ul><li><span class="sect1"><a href="#id-1.2.2"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.2.3"><span class="title-name">Improving the documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.2.4"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.2.5"><span class="title-name">Support</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-introduction"><span class="title-number">1 </span><span class="title-name">Introduction</span></a></span><ul><li><span class="sect1"><a href="#sec-intro-provides"><span class="title-number">1.1 </span><span class="title-name">Components provided</span></a></span></li><li><span class="sect1"><a href="#sec-introduction-hw"><span class="title-number">1.2 </span><span class="title-name">Hardware platform support</span></a></span></li><li><span class="sect1"><a href="#sec-introduction-life"><span class="title-number">1.3 </span><span class="title-name">Support and lifecycle</span></a></span></li><li><span class="sect1"><a href="#sec-introduction-documentation"><span class="title-number">1.4 </span><span class="title-name">Documentation and other information</span></a></span></li></ul></li><li><span class="chapter"><a href="#installation"><span class="title-number">2 </span><span class="title-name">Installation and upgrade</span></a></span><ul><li><span class="sect1"><a href="#sec2-installation-systemrole"><span class="title-number">2.1 </span><span class="title-name">System roles for SUSE Linux Enterprise Server HPC module 15 SP6</span></a></span></li><li><span class="sect1"><a href="#sec2-upgrade"><span class="title-number">2.2 </span><span class="title-name">Upgrading to SUSE Linux Enterprise Server HPC module 15 SP6</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-remote"><span class="title-number">3 </span><span class="title-name">Remote administration</span></a></span><ul><li><span class="sect1"><a href="#sec-remote-genders"><span class="title-number">3.1 </span><span class="title-name">Genders — static cluster configuration database</span></a></span></li><li><span class="sect1"><a href="#sec-remote-pdsh"><span class="title-number">3.2 </span><span class="title-name">pdsh — parallel remote shell program</span></a></span></li><li><span class="sect1"><a href="#sec-remote-powerman"><span class="title-number">3.3 </span><span class="title-name">PowerMan — centralized power control for clusters</span></a></span></li><li><span class="sect1"><a href="#sec-remote-munge"><span class="title-number">3.4 </span><span class="title-name">MUNGE authentication</span></a></span></li><li><span class="sect1"><a href="#sec-remote-mrsh"><span class="title-number">3.5 </span><span class="title-name">mrsh/mrlogin — remote login using MUNGE authentication</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-nodes"><span class="title-number">4 </span><span class="title-name">Hardware</span></a></span><ul><li><span class="sect1"><a href="#sec-nodes-cpuid"><span class="title-number">4.1 </span><span class="title-name">cpuid</span></a></span></li><li><span class="sect1"><a href="#sec-nodes-hwloc"><span class="title-number">4.2 </span><span class="title-name">hwloc — portable abstraction of hierarchical architectures for high-performance computing</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-slurm"><span class="title-number">5 </span><span class="title-name">Slurm — utility for HPC workload management</span></a></span><ul><li><span class="sect1"><a href="#sec-scheduler-slurm"><span class="title-number">5.1 </span><span class="title-name">Installing Slurm</span></a></span></li><li><span class="sect1"><a href="#sec-slurm-admin-commands"><span class="title-number">5.2 </span><span class="title-name">Slurm administration commands</span></a></span></li><li><span class="sect1"><a href="#sec-slurm-upgrade"><span class="title-number">5.3 </span><span class="title-name">Upgrading Slurm</span></a></span></li><li><span class="sect1"><a href="#sec-slurm-faq"><span class="title-number">5.4 </span><span class="title-name">Frequently asked questions</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-monitoring"><span class="title-number">6 </span><span class="title-name">Monitoring and logging</span></a></span><ul><li><span class="sect1"><a href="#sec-remote-conman"><span class="title-number">6.1 </span><span class="title-name">ConMan — the console manager</span></a></span></li><li><span class="sect1"><a href="#sec-monitoring-clusters-prometheus"><span class="title-number">6.2 </span><span class="title-name">Monitoring HPC clusters with Prometheus and Grafana</span></a></span></li><li><span class="sect1"><a href="#sec-monitoring-ras"><span class="title-number">6.3 </span><span class="title-name">rasdaemon — utility to log RAS error tracings</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-compute"><span class="title-number">7 </span><span class="title-name">HPC user libraries</span></a></span><ul><li><span class="sect1"><a href="#sec-compute-lmod"><span class="title-number">7.1 </span><span class="title-name">Lmod — Lua-based environment modules</span></a></span></li><li><span class="sect1"><a href="#sec-compiler"><span class="title-number">7.2 </span><span class="title-name">GNU Compiler Toolchain Collection for HPC</span></a></span></li><li><span class="sect1"><a href="#sec-compute-lib"><span class="title-number">7.3 </span><span class="title-name">High Performance Computing libraries</span></a></span></li><li><span class="sect1"><a href="#FileFormat"><span class="title-number">7.4 </span><span class="title-name">File format libraries</span></a></span></li><li><span class="sect1"><a href="#sec1-MPI-libs"><span class="title-number">7.5 </span><span class="title-name">MPI libraries</span></a></span></li><li><span class="sect1"><a href="#sec1-packages-profiling-benchmark"><span class="title-number">7.6 </span><span class="title-name">Profiling and benchmarking libraries and tools</span></a></span></li><li><span class="sect1"><a href="#sec-environment-containers"><span class="title-number">7.7 </span><span class="title-name">Creating environment containers with Apptainer</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-spack"><span class="title-number">8 </span><span class="title-name">Spack package management tool</span></a></span><ul><li><span class="sect1"><a href="#spack-installation"><span class="title-number">8.1 </span><span class="title-name">Installing Spack</span></a></span></li><li><span class="sect1"><a href="#spack-simple-example"><span class="title-number">8.2 </span><span class="title-name">Using Spack: simple example with netcdf-cxx4</span></a></span></li><li><span class="sect1"><a href="#spack-complex-example"><span class="title-number">8.3 </span><span class="title-name">Using Spack: complex example with mpich</span></a></span></li><li><span class="sect1"><a href="#spack-compiler"><span class="title-number">8.4 </span><span class="title-name">Using a specific compiler</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-dolly"><span class="title-number">9 </span><span class="title-name">Dolly clone tool</span></a></span><ul><li><span class="sect1"><a href="#dolly-cloning-process"><span class="title-number">9.1 </span><span class="title-name">Dolly cloning process</span></a></span></li><li><span class="sect1"><a href="#using-dolly"><span class="title-number">9.2 </span><span class="title-name">Using dolly </span></a></span></li><li><span class="sect1"><a href="#dolly-configuration-file"><span class="title-number">9.3 </span><span class="title-name">Dolly configuration file</span></a></span></li><li><span class="sect1"><a href="#dolly-support"><span class="title-number">9.4 </span><span class="title-name">Dolly limitations</span></a></span></li></ul></li><li><span class="appendix"><a href="#id-1.12"><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="#id-1.12.4"><span class="title-number">A.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#fig-dolly-cloning-process"><span class="number">9.1 </span><span class="name">Dolly cloning process</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-dolly-server-verbose-output"><span class="number">9.1 </span><span class="name">Dolly server verbose output</span></a></span></li><li><span class="example"><a href="#dolly-client-verbose-output"><span class="number">9.2 </span><span class="name">Dolly client verbose output</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.1.6"><p>
  Copyright © 2020–2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section xml:lang="en" class="preface" id="preface-administration" data-id-title="Preface"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Preface</span></span> <a title="Permalink" class="permalink" href="#preface-administration">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/preface_administration.xml" title="Edit source document"> </a></div></div></div></div></div><section xml:lang="en" class="sect1" id="id-1.2.2" data-id-title="Available documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Available documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.2.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_available_doc.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.2.2.3.1"><span class="term">Online documentation</span></dt><dd><p>
     Our documentation is available online at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
     Browse or download the documentation in various formats.
    </p><div id="id-1.2.2.3.1.2.2" data-id-title="Latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Latest updates</div><p>
      The latest updates are usually available in the English-language version of this documentation.
     </p></div></dd><dt id="id-1.2.2.3.2"><span class="term">SUSE Knowledgebase</span></dt><dd><p>
     If you have run into an issue, also check out the Technical Information
     Documents (TIDs) that are available online at <a class="link" href="https://www.suse.com/support/kb/" target="_blank">https://www.suse.com/support/kb/</a>.
     Search the SUSE Knowledgebase for known solutions driven by customer need.
     </p></dd><dt id="id-1.2.2.3.3"><span class="term">Release notes</span></dt><dd><p>
     For release notes, see
     <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
    </p></dd><dt id="id-1.2.2.3.4"><span class="term">In your system</span></dt><dd><p>
      For offline use, the release notes are also available under
      <code class="filename">/usr/share/doc/release-notes</code> on your system.
      The documentation for individual packages is available at
      <code class="filename">/usr/share/doc/packages</code>.
    </p><p>
      Many commands are also described in their <span class="emphasis"><em>manual
      pages</em></span>. To view them, run <code class="command">man</code>, followed
      by a specific command name. If the <code class="command">man</code> command is
      not installed on your system, install it with <code class="command">sudo zypper
      install man</code>.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.2.3" data-id-title="Improving the documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Improving the documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.2.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_feedback.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Your feedback and contributions to this documentation are welcome.
  The following channels for giving feedback are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.2.3.4.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.2.3.4.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
    </p><p>
     To simplify this process, click the <span class="guimenu">Report
     an issue</span> icon next to a headline in the HTML
     version of this document. This preselects the right product and
     category in Bugzilla and adds a link to the current section.
     You can start typing your bug report right away.
    </p><p>
     A Bugzilla account is required.
    </p></dd><dt id="id-1.2.3.4.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, click the <span class="guimenu">Edit source
     document</span> icon next to a headline in the HTML version of
     this document. This will take you to the source code on GitHub, where you
     can open a pull request.</p><p>
     A GitHub account is required.
    </p><div id="id-1.2.3.4.3.2.3" data-id-title="Edit source document only available for English" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="guimenu">Edit source document</span> only available for English</div><p>
      The <span class="guimenu">Edit source document</span> icons are only available for the
      English version of each document. For all other languages, use the
      <span class="guimenu">Report an issue</span> icons instead.
     </p></div><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README.
    </p></dd><dt id="id-1.2.3.4.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.2.4" data-id-title="Documentation conventions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span></span> <a title="Permalink" class="permalink" href="#id-1.2.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_convention.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of a user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. You can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as a non-privileged user:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands can be split into two or multiple lines by a backslash character
    (<code class="literal">\</code>) at the end of a line. The backslash informs the shell that
    the command invocation will continue after the line's end:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">echo</code> a b \
c d</pre></div></li><li class="listitem"><p>
     A code block that shows both the command (preceded by a prompt)
     and the respective output returned by the shell:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code>
output</pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.2.4.4.15.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Warning notice</div><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.2.4.4.15.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Important notice</div><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.2.4.4.15.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Note notice</div><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.2.4.4.15.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Tip notice</div><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.2.4.4.16.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.2.4.4.16.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.2.5" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#id-1.2.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Find the support statement for SUSE Linux Enterprise High Performance Computing and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.2.5.5" data-id-title="Support statement for SUSE Linux Enterprise High Performance Computing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Linux Enterprise High Performance Computing</span></span> <a title="Permalink" class="permalink" href="#id-1.2.5.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offers available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.2.5.5.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.2.5.5.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate a problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.2.5.5.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Linux Enterprise High Performance Computing is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.2.5.6" data-id-title="Technology previews"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span></span> <a title="Permalink" class="permalink" href="#id-1.2.5.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback.
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or otherwise
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a>.
  </p></section></section></section><section xml:lang="en" class="chapter" id="cha-introduction" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="#cha-introduction">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/introduction.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The HPC module for SUSE Linux Enterprise Server is a highly scalable, high-performance parallel computing platform
    for modeling, simulation, and advanced analytics workloads.
   </p></div></div></div></div><section class="sect1" id="sec-intro-provides" data-id-title="Components provided"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">Components provided</span></span> <a title="Permalink" class="permalink" href="#sec-intro-provides">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/introduction.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The HPC module for SUSE Linux Enterprise Server 15 SP6 provides tools and libraries related to High Performance Computing.
   This includes:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A workload manager
    </p></li><li class="listitem"><p>
     Remote and parallel shells
    </p></li><li class="listitem"><p>
     Performance monitoring and measuring tools
    </p></li><li class="listitem"><p>
     A serial-console monitoring tool
    </p></li><li class="listitem"><p>
     A cluster power management tool
    </p></li><li class="listitem"><p>
      A tool for discovering the machine hardware topology
     </p></li><li class="listitem"><p>
      System monitoring
     </p></li><li class="listitem"><p>
      A tool for monitoring memory errors
     </p></li><li class="listitem"><p>
      A tool for determining the CPU model and its capabilities (x86-64 only)
     </p></li><li class="listitem"><p>
     A user-extensible heap manager capable of distinguishing between different
     kinds of memory (x86-64 only)
    </p></li><li class="listitem"><p>
     Various MPI implementations
    </p></li><li class="listitem"><p>
     Serial and parallel computational libraries providing common standards,
     such as BLAS, LAPACK, and others
    </p></li><li class="listitem"><p>
     Serial and parallel libraries for the HDF5 file format
    </p></li></ul></div></section><section class="sect1" id="sec-introduction-hw" data-id-title="Hardware platform support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">Hardware platform support</span></span> <a title="Permalink" class="permalink" href="#sec-introduction-hw">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/introduction.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The HPC module is available for SUSE Linux Enterprise Server 15 SP6 on the
   Intel 64/AMD64 (x86-64) and AArch64 platforms.
  </p></section><section class="sect1" id="sec-introduction-life" data-id-title="Support and lifecycle"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">Support and lifecycle</span></span> <a title="Permalink" class="permalink" href="#sec-introduction-life">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/introduction.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The HPC module is supported throughout the lifecycle of
   SUSE Linux Enterprise Server 15 SP6, including the two lifecycle extensions, <span class="emphasis"><em>Extended Service
   Overlap Support</em></span> (ESPOS) and <span class="emphasis"><em>Long Term Support
   Service</em></span> (LTSS). Any released
   package is fully maintained and supported until the availability of the next
   release.
  </p><p>
   For more information, see the Support Policy page at
   <a class="link" href="https://www.suse.com/support/policy.html" target="_blank">https://www.suse.com/support/policy.html</a>.
  </p></section><section class="sect1" id="sec-introduction-documentation" data-id-title="Documentation and other information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">Documentation and other information</span></span> <a title="Permalink" class="permalink" href="#sec-introduction-documentation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/introduction.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Read the README files on the media.</p></li><li class="listitem"><p>
      Get detailed change log information about a particular package
      from the RPM (where
      <code class="filename"><em class="replaceable">FILENAME</em>.rpm</code>
      is the name of the RPM):
    </p><div class="verbatim-wrap"><pre class="screen">rpm --changelog -qp <em class="replaceable">FILENAME</em>.rpm</pre></div></li><li class="listitem"><p>
     Check the <code class="filename">ChangeLog</code> file in the top level of the
     media for a chronological log of all changes made to the updated
     packages.
    </p></li><li class="listitem"><p>
     The most recent version of the release notes is always available at
     <a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a>.
    </p></li><li class="listitem"><p>
     The most recent version of this documentation is always available at
     <a class="link" href="https://documentation.suse.com/" target="_blank">https://documentation.suse.com/</a>.
    </p></li></ul></div></section></section><section xml:lang="en" class="chapter" id="installation" data-id-title="Installation and upgrade"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Installation and upgrade</span></span> <a title="Permalink" class="permalink" href="#installation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/installation.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The HPC module for SUSE Linux Enterprise Server comes with preconfigured system roles. These
    roles provide a set of preselected packages typical for the specific role,
    and an installation workflow that configures the system to make
    the best use of system resources based on a typical use case for the role.
   </p></div></div></div></div><section class="sect1" id="sec2-installation-systemrole" data-id-title="System roles for SUSE Linux Enterprise Server HPC module 15 SP6"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">System roles for SUSE Linux Enterprise Server HPC module 15 SP6</span></span> <a title="Permalink" class="permalink" href="#sec2-installation-systemrole">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can choose specific roles for the system based on modules selected
    during the installation process. When the HPC module is enabled,
    the following roles are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.1"><span class="term">HPC management server (head node)</span></dt><dd><p>
       This role includes the following features:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Uses Btrfs as the default root file system
        </p></li><li class="listitem"><p>
         Includes HPC-enabled libraries
        </p></li><li class="listitem"><p>
         Disables the firewall and Kdump services
        </p></li><li class="listitem"><p>
         Installs a controller for the Slurm workload manager
        </p></li><li class="listitem"><p>
         Mounts a large scratch partition to <code class="filename">/var/tmp</code>
        </p></li></ul></div></dd><dt id="id-1.4.3.3.2"><span class="term">HPC compute node</span></dt><dd><p>
       This role includes the following features:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Based on the minimal setup configuration
        </p></li><li class="listitem"><p>
         Uses XFS as the default root file system
        </p></li><li class="listitem"><p>
         Includes HPC-enabled libraries
        </p></li><li class="listitem"><p>
         Disables firewall and Kdump services
        </p></li><li class="listitem"><p>
         Installs a client for the Slurm workload manager
        </p></li><li class="listitem"><p>
         Does not create a separate <code class="literal">/home</code> partition
        </p></li><li class="listitem"><p>
         Mounts a large scratch partition to <code class="filename">/var/tmp</code>
        </p></li></ul></div></dd><dt id="id-1.4.3.3.3"><span class="term">HPC development node</span></dt><dd><p>
       This role includes the following features:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Includes HPC-enabled libraries
        </p></li><li class="listitem"><p>
         Adds compilers and development toolchains
        </p></li></ul></div></dd></dl></div><p>
    The scratch partition <code class="literal">/var/tmp/</code> is only created if
    there is sufficient space available on the installation medium (minimum
    32 GB).
   </p><p>
    The environment module <code class="literal">Lmod</code> is installed for all
    roles. It is required at build time and runtime of the system. For more
    information, see <a class="xref" href="#sec-compute-lmod" title="7.1. Lmod — Lua-based environment modules">Section 7.1, “Lmod — Lua-based environment modules”</a>.
   </p><p>
    All libraries specifically built for HPC are installed under
    <code class="literal">/usr/lib/hpc</code>. They are not part of the standard search
    path, so the <code class="literal">Lmod</code> environment module system is
    required.
   </p><p>
    <code class="literal">MUNGE</code> authentication is installed for all roles. MUNGE
    keys are generated and must be copied to all nodes in the
    cluster. For more information, see <a class="xref" href="#sec-remote-munge" title="3.4. MUNGE authentication">Section 3.4, “MUNGE authentication”</a>.
   </p><p>
    The system roles are only available for new installations of the HPC module for SUSE Linux Enterprise Server.
   </p></section><section class="sect1" id="sec2-upgrade" data-id-title="Upgrading to SUSE Linux Enterprise Server HPC module 15 SP6"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Upgrading to SUSE Linux Enterprise Server HPC module 15 SP6</span></span> <a title="Permalink" class="permalink" href="#sec2-upgrade">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In version 15 SP6, SUSE Linux Enterprise High Performance Computing (SLE HPC) was removed as a separate product, and is now available as the HPC module for SUSE Linux Enterprise Server (SLES).
   </p><p>
     You can upgrade from SLE HPC 15 SP5 to SLES 15 SP6 plus the HPC module.
   </p></section></section><section xml:lang="en" class="chapter" id="cha-remote" data-id-title="Remote administration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Remote administration</span></span> <a title="Permalink" class="permalink" href="#cha-remote">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    High Performance Computing clusters usually consist of a small set of identical compute
    nodes. However, large clusters could consist of thousands of machines.
    This chapter describes tools to help manage the compute nodes in a cluster.
   </p></div></div></div></div><section class="sect1" id="sec-remote-genders" data-id-title="Genders — static cluster configuration database"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Genders — static cluster configuration database</span></span> <a title="Permalink" class="permalink" href="#sec-remote-genders">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="emphasis"><em>Genders</em></span> is a static cluster configuration database used
   for configuration management. It allows grouping and addressing sets of
   nodes by attributes, and is used by a variety of tools. The Genders
   database is a text file that is usually replicated on each node in a
   cluster.
  </p><p>
   Perl, Python, Lua, C, and C++ bindings are supplied with Genders. Each
   package provides <code class="command">man</code> pages or other documentation which
   describes the APIs.
  </p><section class="sect2" id="sec-genders-db" data-id-title="Genders database format"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.1.1 </span><span class="title-name">Genders database format</span></span> <a title="Permalink" class="permalink" href="#sec-genders-db">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Genders database in SUSE Linux Enterprise High Performance Computing is a plain-text file called
   <code class="filename">/etc/genders</code>. It contains a list of node names with
   their attributes. Each line of the database can have one of the following
   formats.
  </p><div class="verbatim-wrap"><pre class="screen">nodename                attr[=value],attr[=value],...
nodename1,nodename2,... attr[=value],attr[=value],...
nodenames[A-B]          attr[=value],attr[=value],...</pre></div><p>
   Node names are listed without their domain, and are followed by any number of
   spaces or tabs, then the comma-separated list of attributes. Every
   attribute can optionally have a value. The substitution string
   <code class="literal">%n</code> can be used in an attribute value to represent the node
   name. Node names can be listed on multiple lines, so a node's attributes can
   be specified on multiple lines. However, no single node can have duplicate
   attributes.
  </p><p>
   The attribute list must not contain spaces, and there is no provision for
   continuation lines.  Commas and equals characters (<code class="literal">=</code>) are
   special, and cannot appear in attribute names or values. Comments are
   prefixed with the hash character (<code class="literal">#</code>) and can appear
   anywhere in the file.
  </p><p>
   Ranges for node names can be specified in the form
   <code class="literal">prefix[a-c,n-p,...]</code> as an alternative to explicit lists
   of node names. For example, <code class="literal">node[01-03,06]</code> would specify
   <code class="literal">node01</code>, <code class="literal">node02</code>, <code class="literal">node03</code>,
   and <code class="literal">node06</code>.
  </p></section><section class="sect2" id="sec-nodeattr" data-id-title="Nodeattr usage"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.1.2 </span><span class="title-name">Nodeattr usage</span></span> <a title="Permalink" class="permalink" href="#sec-nodeattr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The command line utility <code class="command">nodeattr</code> can be used to query data
    in the genders file. When the genders file is replicated on all nodes, a query
    can be done without network access. The genders file can be called as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>nodeattr [-q | -n | -s] [-r] attr[=val]</pre></div><p>
   <code class="literal">-q</code> is the default option and prints a list of nodes
   with <code class="literal">attr[=val]</code>.
  </p><p>
   The <code class="literal">-c</code> or <code class="literal">-s</code> options give a
   comma-separated or space-separated list of nodes with
   <code class="literal">attr[=val]</code>.
  </p><p>
   If none of the formatting options are specified, <code class="command">nodeattr</code>
   returns a zero value if the local node has the specified attribute, and
   non-zero otherwise. The <code class="literal">-v</code> option causes any value
   associated with the attribute to go to <code class="literal">stdout</code>. If a node
   name is specified before the attribute, the specified node is queried instead
   of the local node.
  </p><p>
    To print all attributes for a particular node, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>nodeattr -l [node]</pre></div><p>
   If no node parameter is given, all attributes of the local node are printed.
  </p><p>
   To perform a syntax check of the genders database, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>nodeattr [-f genders] -k</pre></div><p>
   To specify an alternative database location, use the option
   <code class="literal">-f</code>.
  </p></section></section><section class="sect1" id="sec-remote-pdsh" data-id-title="pdsh — parallel remote shell program"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">pdsh — parallel remote shell program</span></span> <a title="Permalink" class="permalink" href="#sec-remote-pdsh">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="command">pdsh</code> is a parallel remote shell that can be used
   with multiple back-ends for remote connections. It can run a command on
   multiple machines in parallel.
  </p><p>
   To install <span class="package">pdsh</span>, run the command
   <code class="command">zypper in pdsh</code>.
  </p><p>
   The HPC module for SUSE Linux Enterprise Server supports the back-ends <code class="literal">ssh</code>,
   <code class="literal">mrsh</code>, and <code class="literal">exec</code>. The
   <code class="literal">ssh</code> back-end is the default. Non-default login methods
   can be used by setting the <code class="literal">PDSH_RCMD_TYPE</code>
   environment variable, or by using the <code class="literal">-R</code> command
   argument.
  </p><p>
   When using the <code class="literal">ssh</code> back-end, you must use a
   non-interactive (passwordless) login method.
  </p><p>
   The <code class="literal">mrsh</code> back-end requires the
   <code class="literal">mrshd</code> daemon to be running on the client. The
   <code class="literal">mrsh</code> back-end does not require the use of reserved
   sockets, so it does not suffer from port exhaustion when running commands
   on many machines in parallel. For information about setting up the system to
   use this back-end, see <a class="xref" href="#sec-remote-mrsh" title="3.5. mrsh/mrlogin — remote login using MUNGE authentication">Section 3.5, “mrsh/mrlogin — remote login using MUNGE authentication”</a>.
  </p><p>
   Remote machines can be specified on the command line, or
   <code class="command">pdsh</code> can use a <code class="filename">machines</code> file
   (<code class="filename">/etc/pdsh/machines</code>), <code class="command">dsh</code> (Dancer's
   shell)-style groups or netgroups. It can also target nodes based on the
   currently running Slurm jobs.
  </p><p>
   The different ways to select target hosts are realized by modules. Some
   of these modules provide identical options to <code class="command">pdsh</code>.
   The module loaded first will win and handle the option. Therefore, we
   recommended using a single method and specifying this with
   the <code class="literal">-M</code> option.
  </p><p>
   The <code class="filename">machines</code> file lists all target hosts, one per
   line. The appropriate netgroup can be selected with the
   <code class="literal">-g</code> command line option.
  </p><p>
   The following host-list plugins for <code class="command">pdsh</code> are supported:
   <code class="literal">machines</code>, <code class="literal">slurm</code>,
   <code class="literal">netgroup</code> and <code class="literal">dshgroup</code>.
   Each host-list plugin is provided in a separate package. This avoids
   conflicts between command line options for different plugins which
   happen to be identical, and helps to keep installations small and free
   of unneeded dependencies. Package dependencies have been set to prevent
   the installation of plugins with conflicting command options. To install one
   of the plugins, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo zypper in pdsh-<em class="replaceable">PLUGIN_NAME</em></pre></div><p>
   For more information, see the <code class="command">man</code> page <code class="command">pdsh</code>.
  </p></section><section class="sect1" id="sec-remote-powerman" data-id-title="PowerMan — centralized power control for clusters"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.3 </span><span class="title-name">PowerMan — centralized power control for clusters</span></span> <a title="Permalink" class="permalink" href="#sec-remote-powerman">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   PowerMan can control the following remote power control devices (RPC) from a
   central location:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     local devices connected to a serial port
    </p></li><li class="listitem"><p>
     RPCs listening on a TCP socket
    </p></li><li class="listitem"><p>
     RPCs that are accessed through an external program
    </p></li></ul></div><p>
   The communication to RPCs is controlled by <span class="quote">“<span class="quote">expect</span>”</span>-like
   scripts. For a
   list of currently supported devices, see the configuration file
   <code class="filename">/etc/powerman/powerman.conf</code>.
  </p><p>
   To install PowerMan, run <code class="command">zypper in powerman</code>.
  </p><p>
   To configure PowerMan, include the appropriate device file for your RPC
   (<code class="filename">/etc/powerman/*.dev</code>) in
   <code class="filename">/etc/powerman/powerman.conf</code> and add devices and
   nodes. The device <span class="quote">“<span class="quote">type</span>”</span> needs to match the
   <span class="quote">“<span class="quote">specification</span>”</span> name in one
   of the included device files. The list of <span class="quote">“<span class="quote">plugs</span>”</span> used for
   nodes needs to
   match an entry in the <span class="quote">“<span class="quote">plug name</span>”</span> list.
  </p><p>
   After configuring PowerMan, start its service:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo systemctl start powerman.service</pre></div><p>
   To start PowerMan automatically after every boot, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo systemctl enable powerman.service</pre></div><p>
   Optionally, PowerMan can connect to a remote PowerMan instance. To
   enable this, add the option <code class="literal">listen</code> to
   <code class="filename">/etc/powerman/powerman.conf</code>.
  </p><div id="id-1.5.5.12" data-id-title="Unencrypted transfer" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Unencrypted transfer</div><p>
    When connecting to a remote PowerMan instance, data is transferred
    unencrypted. Therefore, use this feature only if the network is
    appropriately secured.
   </p></div></section><section class="sect1" id="sec-remote-munge" data-id-title="MUNGE authentication"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.4 </span><span class="title-name">MUNGE authentication</span></span> <a title="Permalink" class="permalink" href="#sec-remote-munge">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   MUNGE allows for secure communications between different machines that
   share the same secret key. The most common use case is the Slurm workload
   manager, which uses MUNGE for the encryption of its messages. Another use
   case is authentication for the parallel shell mrsh.
  </p><section class="sect2" id="sec-setting-up-munge" data-id-title="Setting up MUNGE authentication"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.4.1 </span><span class="title-name">Setting up MUNGE authentication</span></span> <a title="Permalink" class="permalink" href="#sec-setting-up-munge">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    MUNGE uses UID/GID values to uniquely identify and authenticate users,
    so you must ensure that users who will authenticate across a network
    have matching UIDs and GIDs across all nodes.
   </p><p>
    MUNGE credentials have a limited time-to-live, so you must ensure that the
    time is synchronized across the entire cluster.
   </p><p>
    MUNGE is installed with the command <code class="command">zypper in munge</code>.
    This also installs further required packages. A separate
    <span class="package">munge-devel</span> package is available to build applications
    that require MUNGE authentication.
   </p><p>
    When installing the <span class="package">munge</span> package, a new key is generated
    on every system. However, the entire cluster needs to use the same MUNGE
    key. Therefore, you must securely copy the MUNGE key from one system to
    all the other nodes in the cluster. You can accomplish this by using
    <code class="command">pdsh</code> with SSH. Ensure that the key is only readable
    by the <code class="literal">munge</code> user (permissions mask
    <code class="literal">0400</code>).
   </p><div class="procedure" id="pro-remote-munge" data-id-title="Setting up MUNGE authentication"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3.1: </span><span class="title-name">Setting up MUNGE authentication </span></span><a title="Permalink" class="permalink" href="#pro-remote-munge">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the server where MUNGE is installed, check the permissions, owner,
      and file type of the key file <code class="filename">/etc/munge/munge.key</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo stat --format "%F %a %G %U %n" /etc/munge/munge.key</pre></div><p>
      The settings should be as follows:
     </p><div class="verbatim-wrap"><pre class="screen">400 regular file munge munge /etc/munge/munge.key</pre></div></li><li class="step"><p>
      Calculate the MD5 sum of <code class="filename">munge.key</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo md5sum /etc/munge/munge.key</pre></div></li><li class="step"><p>
      Copy the key to the listed nodes using <code class="command">pdcp</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>pdcp -R ssh -w <em class="replaceable">NODELIST</em> /etc/munge/munge.key /etc/munge/munge.key</pre></div></li><li class="step"><p>
      Check the key settings on the remote nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>pdsh -R ssh -w <em class="replaceable">HOSTLIST</em> stat --format \"%F %a %G %U %n\" /etc/munge/munge.key
<code class="prompt user">&gt; </code>pdsh -R ssh -w <em class="replaceable">HOSTLIST</em> md5sum /etc/munge/munge.key</pre></div><p>
      Ensure that they match the settings on the MUNGE server.
     </p></li></ol></div></div></section><section class="sect2" id="sec-enabling-and-starting-munge" data-id-title="Enabling and starting MUNGE"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.4.2 </span><span class="title-name">Enabling and starting MUNGE</span></span> <a title="Permalink" class="permalink" href="#sec-enabling-and-starting-munge">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <code class="systemitem">munged</code> must be running on all nodes
    that use MUNGE authentication. If MUNGE is used for
    authentication across the network, it needs to run on each side of the
    communications link.
   </p><p>
    To start the service and ensure it is started after every reboot, run
    the following command on each node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo systemctl enable --now munge.service</pre></div><p>
     You can also use <code class="command">pdsh</code> to run this command on multiple
     nodes at once.
   </p></section></section><section class="sect1" id="sec-remote-mrsh" data-id-title="mrsh/mrlogin — remote login using MUNGE authentication"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.5 </span><span class="title-name">mrsh/mrlogin — remote login using MUNGE authentication</span></span> <a title="Permalink" class="permalink" href="#sec-remote-mrsh">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/remote_administration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="emphasis"><em>mrsh</em></span> is a set of remote shell programs using the
   <span class="emphasis"><em>MUNGE</em></span> authentication system instead of reserved ports
   for security.
  </p><p>
   It can be used as a drop-in replacement for <code class="literal">rsh</code> and
   <code class="literal">rlogin</code>.
  </p><p>
   To install mrsh, do the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If only the mrsh client is required (without
     allowing remote login to this machine), use:
     <code class="command">zypper in mrsh</code>.
    </p></li><li class="listitem"><p>
     To allow logging in to a machine, the server must be installed:
     <code class="command">zypper in mrsh-server</code>.
    </p></li><li class="listitem"><p>
     To get a drop-in replacement for <code class="command">rsh</code> and
     <code class="command">rlogin</code>, run: <code class="command">zypper in
     mrsh-rsh-server-compat</code> or <code class="command">zypper in
     mrsh-rsh-compat</code>.
    </p></li></ul></div><p>
   To set up a cluster of machines allowing remote login from each other,
   first follow the instructions for setting up and starting MUNGE
   authentication in <a class="xref" href="#sec-remote-munge" title="3.4. MUNGE authentication">Section 3.4, “MUNGE authentication”</a>. After the MUNGE service
   successfully starts, enable and start <code class="command">mrlogin</code>
   on each machine on which the user will log in:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo systemctl enable mrlogind.socket mrshd.socket
<code class="prompt user">&gt; </code>sudo systemctl start mrlogind.socket mrshd.socket</pre></div><p>
   To start mrsh support at boot, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo systemctl enable munge.service
<code class="prompt user">&gt; </code>sudo systemctl enable mrlogin.service</pre></div><p>
   We do not recommend using mrsh when logged in as the
   user <code class="systemitem">root</code>. This is disabled by
   default. To enable it anyway, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo echo "mrsh" &gt;&gt; /etc/securetty
<code class="prompt user">&gt; </code>sudo echo "mrlogin" &gt;&gt; /etc/securetty</pre></div></section></section><section xml:lang="en" class="chapter" id="cha-nodes" data-id-title="Hardware"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Hardware</span></span> <a title="Permalink" class="permalink" href="#cha-nodes">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/hardware.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter describes tools that can be used to obtain hardware
    infrastructure information for HPC applications.
   </p></div></div></div></div><section class="sect1" id="sec-nodes-cpuid" data-id-title="cpuid"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">cpuid</span></span> <a title="Permalink" class="permalink" href="#sec-nodes-cpuid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/hardware.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="literal">cpuid</code> executes the x86 CPUID instruction and decodes
   and prints the results to <code class="literal">stdout</code>. Its knowledge of Intel, AMD and Cyrix
   CPUs is fairly complete. It specifically targets the Intel Xeon Phi
   architecture.
  </p><p>
   To install <code class="command">cpuid</code>, run <code class="command">zypper in cpuid</code>.
  </p><p>
   For information about runtime options for <code class="command">cpuid</code>, see the
   man page <code class="literal">cpuid(1)</code>.
  </p><p>
   Note that this tool is only available for x86-64.
  </p></section><section class="sect1" id="sec-nodes-hwloc" data-id-title="hwloc — portable abstraction of hierarchical architectures for high-performance computing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">hwloc — portable abstraction of hierarchical architectures for high-performance computing</span></span> <a title="Permalink" class="permalink" href="#sec-nodes-hwloc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/hardware.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="literal">hwloc</code> provides CLI tools and a C API to
   obtain the hierarchical map of key computing elements, such as NUMA
   memory nodes, shared caches, processor packages, processor cores,
   processing units (logical processors or <span class="quote">“<span class="quote">threads</span>”</span>), and I/O
   devices. <code class="literal">hwloc</code> also gathers various attributes such as
   cache and memory information, and is portable across a variety of different
   operating systems and platforms. It can also assemble the
   topologies of multiple machines into a single one, so that applications
   can read the topology of an entire fabric or cluster at once.
  </p><p>
   <code class="literal">lstopo</code> allows the user to obtain the topology
   of a machine or convert topology information obtained on a remote
   machine into one of several output formats. In graphical mode (X11),
   it displays the topology in a window. Other available formats include
   plain text, PDF, PNG, SVG and FIG.
   For more information, see the man pages provided by
   <code class="literal">hwloc</code> and <code class="literal">lstopo</code>.
  </p><p>
   <code class="literal">hwloc</code> features full support for import and export of XML-formatted
   topology files via the <code class="literal">libxml2</code> library.
  </p><p>
   The package <code class="literal">hwloc-devel</code> offers a library that can be
   directly included into external programs. This requires that the
   <code class="literal">libxml2</code> development library (package
   <code class="literal">libxml2-devel</code>) is available when compiling
   <code class="literal">hwloc</code>.
   </p></section></section><section xml:lang="en" class="chapter" id="cha-slurm" data-id-title="Slurm — utility for HPC workload management"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Slurm — utility for HPC workload management</span></span> <a title="Permalink" class="permalink" href="#cha-slurm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    <span class="emphasis"><em>Slurm</em></span> is a workload manager for managing compute jobs
    on High Performance Computing clusters. It can start multiple jobs on a
    single node, or a single job on multiple nodes. Additional components can be
    used for advanced scheduling and accounting.
   </p><p>
    The mandatory components of Slurm are the control daemon
    <span class="emphasis"><em>slurmctld</em></span>, which handles job scheduling, and the
    Slurm daemon <span class="emphasis"><em>slurmd</em></span>, responsible for launching compute
    jobs. Nodes running <code class="command">slurmctld</code> are called
    <span class="emphasis"><em>management servers</em></span> and nodes running
    <code class="command">slurmd</code> are called <span class="emphasis"><em>compute nodes</em></span>.
   </p><p>
    Additional components are a secondary <span class="emphasis"><em>slurmctld</em></span> acting
    as a standby server for a failover, and the Slurm database daemon
    <span class="emphasis"><em>slurmdbd</em></span>, which stores the job history and user
    hierarchy.
   </p><p>
    For further documentation, see the
    <a class="link" href="https://slurm.schedmd.com/quickstart_admin.html" target="_blank">Quick
    Start Administrator Guide</a> and
    <a class="link" href="https://slurm.schedmd.com/quickstart.html" target="_blank"> Quick
    Start User Guide</a>. There is further in-depth documentation on the
    <a class="link" href="https://slurm.schedmd.com/documentation.html" target="_blank">Slurm
    documentation page</a>.
   </p></div></div></div></div><section class="sect1" id="sec-scheduler-slurm" data-id-title="Installing Slurm"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Installing Slurm</span></span> <a title="Permalink" class="permalink" href="#sec-scheduler-slurm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   These instructions describe a minimal installation of Slurm with one
   management server and multiple compute nodes.
  </p><section class="sect2" id="sec-slurm-minimal" data-id-title="Minimal installation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1.1 </span><span class="title-name">Minimal installation</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-minimal">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.7.3.3.2" data-id-title="Make sure of consistent UIDs and GIDs for Slurms accounts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Make sure of consistent UIDs and GIDs for Slurm's accounts</div><p>
     For security reasons, Slurm does not run as the user
     <code class="systemitem">root</code>, but under its own
     user. It is important that the user
     <code class="systemitem">slurm</code> has the same UID/GID
     across all nodes of the cluster.
    </p><p>
     If this user/group does not exist, the package <span class="package">slurm</span>
     creates this user and group when it is installed. However, this does not
     guarantee that the generated UIDs/GIDs will be identical on all systems.
    </p><p>
     Therefore, we strongly advise you to create the user/group
     <code class="systemitem">slurm</code> before installing
     <span class="package">slurm</span>. If you are using a network directory service
     such as LDAP for user and group management, you can use it to provide the
     <code class="systemitem">slurm</code> user/group as well.
    </p><p>
     It is strongly recommended that all compute nodes share common user
     home directories. These should be provided through network storage.
    </p></div><div class="procedure" id="pro-installing-slurm" data-id-title="Installing the Slurm packages"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Installing the Slurm packages </span></span><a title="Permalink" class="permalink" href="#pro-installing-slurm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the management server, install the <span class="package">slurm</span> package with the
      command <code class="command">zypper in slurm</code>.
     </p></li><li class="step"><p>
      On the compute nodes, install the <span class="package">slurm-node</span> package
      with the command <code class="command">zypper in slurm-node</code>.
     </p></li><li class="step"><p>
      On the management server and the compute nodes, the package
      <span class="package">munge</span> is installed automatically. Configure, enable
      and start MUNGE on the management server and compute nodes as
      described in <a class="xref" href="#sec-remote-munge" title="3.4. MUNGE authentication">Section 3.4, “MUNGE authentication”</a>. Ensure that the same
      <code class="literal">munge</code> key is shared across all nodes.
     </p></li></ol></div></div><div id="id-1.7.3.3.4" data-id-title="Automatically opened ports" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Automatically opened ports</div><p>
     Installing the <span class="package">slurm</span> package automatically opens the TCP
     ports 6817, 6818, and 6819.
    </p></div><div class="procedure" id="pro-configuring-slurm" data-id-title="Configuring Slurm"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.2: </span><span class="title-name">Configuring Slurm </span></span><a title="Permalink" class="permalink" href="#pro-configuring-slurm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the management server, edit the main configuration file
      <code class="filename">/etc/slurm/slurm.conf</code>:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Configure the parameter
        <code class="literal">SlurmctldHost=<em class="replaceable">SLURMCTLD_HOST</em></code>
        with the host name of the management server.
       </p><p>
        To find the correct host name, run <code class="command">hostname -s</code>
        on the management server.
       </p></li><li class="step"><p>
        Under the <code class="literal">COMPUTE NODES</code> section, add the following
        lines to define the compute nodes:
       </p><div class="verbatim-wrap"><pre class="screen">NodeName=<em class="replaceable">NODE_LIST</em> State=UNKNOWN
PartitionName=normal Nodes=<em class="replaceable">NODE_LIST</em> Default=YES MaxTime=24:00:00 State=UP</pre></div><p>
        Replace <em class="replaceable">NODE_LIST</em> with the host names
        of the compute nodes, either comma-separated or as a range (for example:
        <code class="literal">node[1-100]</code>).
       </p><p>
        The <code class="literal">NodeName</code> line also allows specifying additional
        parameters for the nodes, such as
        <code class="literal">Boards</code>, <code class="literal">SocketsPerBoard</code>
        <code class="literal">CoresPerSocket</code>, <code class="literal">ThreadsPerCore</code>,
        or <code class="literal">CPU</code>. The actual values of these can be
        obtained by running the following command on the compute nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">node1 # </code>slurmd -C</pre></div></li></ol></li><li class="step"><p>
      Copy the modified configuration file
      <code class="filename">/etc/slurm/slurm.conf</code> from the management server to all
      compute nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>scp /etc/slurm/slurm.conf <em class="replaceable">COMPUTE_NODE</em>:/etc/slurm/</pre></div></li><li class="step"><p>
      On the management server, start <code class="systemitem">slurmctld</code>
      and enable it so that it starts on every boot:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl enable --now slurmctld.service</pre></div></li><li class="step"><p>
      On each compute node, start <code class="systemitem">slurmd</code>
      and enable it so that it starts on every boot:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">node1 # </code>systemctl enable --now slurmd.service</pre></div></li></ol></div></div><div class="procedure" id="pro-testing-slurm-installation" data-id-title="Testing the Slurm installation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.3: </span><span class="title-name">Testing the Slurm installation </span></span><a title="Permalink" class="permalink" href="#pro-testing-slurm-installation">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check the status and availability of the compute nodes by running the
      <code class="command">sinfo</code> command. You should see output similar to
      the following:
     </p><div class="verbatim-wrap"><pre class="screen">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 1-00:00:00      2   idle node[01-02]</pre></div><p>
      If the node state is not <code class="literal">idle</code>, see
      <a class="xref" href="#sec-slurm-faq" title="5.4. Frequently asked questions">Section 5.4, “Frequently asked questions”</a>.
     </p></li><li class="step"><p>
      Test the Slurm installation by running the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>srun sleep 30</pre></div><p>
      This runs the <code class="command">sleep</code> command on a free compute node
      for 30 seconds.
     </p><p>
      In another shell, run the <code class="command">squeue</code> command during the
      30 seconds that the compute node is asleep. You should see output similar
      to the following:
     </p><div class="verbatim-wrap"><pre class="screen">JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    1    normal    sleep     root  R       0:05      1 node02</pre></div></li><li class="step"><p>
      Create the following shell script and save it as
      <code class="filename">sleeper.sh</code>:
     </p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
echo "started at $(date)"
sleep 30
echo "finished at $(date)"</pre></div><p>
      Run the shell script in the queue:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>sbatch sleeper.sh</pre></div><p>
      The shell script is executed when enough resources are available,
      and the output is stored in the file <code class="filename">slurm-${JOBNR}.out</code>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-slurm-slurmdbd" data-id-title="Installing the Slurm database"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1.2 </span><span class="title-name">Installing the Slurm database</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-slurmdbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In a minimal installation, Slurm only stores pending and running
    jobs. To store finished and failed job data, the storage plugin
    must be installed and enabled. You can also enable
    <span class="emphasis"><em>completely fair scheduling</em></span>, which replaces FIFO
    (first in, first out) scheduling with algorithms that calculate the job
    priority in a queue in dependence of the job which a user has run in the history.
    
   </p><p>
    The Slurm database has two components: the <code class="literal">slurmdbd</code>
    daemon itself, and an SQL database. MariaDB is recommended. The
    database can be installed on the same node that runs <code class="literal">slurmdbd</code>,
    or on a separate node. For a minimal setup, all these services run on the
    management server.
   </p><div class="procedure" id="id-1.7.3.4.4" data-id-title="Install slurmdbd"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.4: </span><span class="title-name">Install <span class="package">slurmdbd</span> </span></span><a title="Permalink" class="permalink" href="#id-1.7.3.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.7.3.4.4.2" data-id-title="MariaDB" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: MariaDB</div><p>
      If you want to use an external SQL database (or you already have a
      database installed on the management server), you can skip
      <a class="xref" href="#st-install-mariadb" title="Step 1">Step 1</a> and <a class="xref" href="#st-start-mariadb" title="Step 2">Step 2</a>.
     </p></div><ol class="procedure" type="1"><li class="step" id="st-install-mariadb"><p>
      Install the MariaDB SQL database with <code class="command">zypper in
      mariadb</code>.
     </p></li><li class="step" id="st-start-mariadb"><p>
      Start and enable MariaDB:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl enable --now mariadb</pre></div></li><li class="step"><p>
      Secure the database with the command
      <code class="command">mysql_secure_installation</code>.
     </p></li><li class="step"><p>
      Connect to the SQL database, for example with the command
      <code class="command">mysql -u root -p</code>.
     </p></li><li class="step" id="sec-sum-sqldb"><p>
      Create the Slurm user and the database with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">sql &gt; </code>create user 'slurm'@'localhost' identified by '<em class="replaceable">PASSWORD</em>';
<code class="prompt user">sql &gt; </code>grant all on slurm_acct_db.* TO 'slurm'@'localhost';
<code class="prompt user">sql &gt; </code>create database slurm_acct_db;</pre></div><p>
      After these steps are complete, exit the database.
     </p></li><li class="step"><p>
      Install the <span class="package">slurmdbd</span> package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>zypper in slurm-slurmdbd</pre></div></li><li class="step"><p>
      Edit the <code class="filename">/etc/slurm/slurmdbd.conf</code> file so that the
      daemon can access the database. Change the following line to the password
      that you used in <a class="xref" href="#sec-sum-sqldb" title="Step 5">Step 5</a>:
     </p><div class="verbatim-wrap"><pre class="screen">StoragePass=password</pre></div><p>
       If you chose another location or user for the SQL database, you must also
      modify the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">StorageUser=slurm
DbdAddr=localhost
DbdHost=localhost</pre></div></li><li class="step"><p>
      Start and enable <code class="literal">slurmdbd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl enable --now slurmdbd</pre></div><p>
      The first start of <code class="literal">slurmdbd</code> will take some time.
     </p></li><li class="step"><p>
      To enable accounting, edit the <code class="filename">/etc/slurm/slurm.conf</code>
      file to add the connection between <code class="literal">slurmctld</code> and the
      <code class="literal">slurmdbd</code> daemon. Ensure that the following lines
      appear as shown:
     </p><div class="verbatim-wrap"><pre class="screen">JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost</pre></div><div id="id-1.7.3.4.4.11.3" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
       This example assumes that <code class="literal">slurmdbd</code> is
      running on the same node as <code class="literal">slurmctld</code>. If not, change
      <code class="literal">localhost</code> to the host name or IP address of the node
      where <code class="literal">slurmdbd</code> is running.
      </p></div></li><li class="step"><p>
      Ensure that a table for the cluster is added to the database.
      Otherwise, no accounting information can be written to the database.
      To add a cluster table, run
      <code class="command">sacctmgr -i add cluster <em class="replaceable">CLUSTERNAME</em></code>.
     </p></li><li class="step"><p>
      Restart <code class="literal">slurmctld</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl restart slurmctld</pre></div></li><li class="step"><p><span class="step-optional">(Optional)</span> 
      By default, Slurm does not take any group membership into account, and
      the system groups cannot be mapped to Slurm. Group creation and
      membership must be managed via the command line tool
      <code class="command">sacctmgr</code>. You can have a group
      hierarchy, and users can be part of several groups.
     </p><p>
      The following example creates an umbrella group <code class="literal">bavaria</code>
      for two subgroups called <code class="literal">nuremberg</code> and <code class="literal">munich</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>sacctmgr add account bavaria \
Description="umbrella group for subgroups" Organization=bavaria</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>sacctmgr add account nuremberg,munich parent=bavaria \
Description="subgroup" Organization=bavaria</pre></div><p>
      The following example adds a user called <code class="literal">tux</code> to the
      subgroup <code class="literal">nuremberg</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>sacctmgr add user tux Account=nuremberg</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-slurm-admin-commands" data-id-title="Slurm administration commands"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Slurm administration commands</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-admin-commands">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section lists some useful options for common Slurm commands. For more
   information and a full list of options, see the <code class="command">man</code> page
   for each command. For more Slurm commands, see
   <a class="link" href="https://slurm.schedmd.com/man_index.html" target="_blank">https://slurm.schedmd.com/man_index.html</a>.
  </p><section class="sect2" id="sec-slurm-sconfigure" data-id-title="scontrol"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">scontrol</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-sconfigure">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The command <code class="command">scontrol</code> is used to show and update the
    entities of Slurm, such as the state of the compute nodes or compute jobs.
    It can also be used to reboot or to propagate configuration changes to the
    compute nodes.
   </p><p>
    Useful options for this command are <code class="option">--details</code>, which
    prints more verbose output, and <code class="option">--oneliner</code>, which forces
    the output onto a single line, which is more useful in shell scripts.
   </p><p>
    For more information, see <code class="command">man scontrol</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.3.5.1"><span class="term"><code class="command">scontrol show <em class="replaceable">ENTITY</em></code></span></dt><dd><p>
       Displays the state of the specified <em class="replaceable">ENTITY</em>.
      </p></dd><dt id="id-1.7.4.3.5.2"><span class="term"><code class="command">scontrol update <em class="replaceable">SPECIFICATION</em></code></span></dt><dd><p>
       Updates the <em class="replaceable">SPECIFICATION</em> like the compute node
       or compute node state. Useful <em class="replaceable">SPECIFICATION</em>
       states that can be set for compute nodes include:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.3.5.2.2.2.1"><span class="term"><code class="command">nodename=<em class="replaceable">NODE</em> state=down reason=<em class="replaceable">REASON</em></code></span></dt><dd><p>
          Removes all jobs from the compute node, and aborts any jobs already
          running on the node.
         </p></dd><dt id="id-1.7.4.3.5.2.2.2.2"><span class="term"><code class="command">nodename=<em class="replaceable">NODE</em> state=drain reason=<em class="replaceable">REASON</em></code></span></dt><dd><p>
          Drains the compute node so that no <span class="emphasis"><em>new</em></span> jobs
          can be scheduled on it, but does not end compute jobs already
          running on the compute node. <em class="replaceable">REASON</em> can
          be any string. The compute node stays in the <code class="literal">drained</code>
          state and must be returned to the <code class="literal">idle</code> state manually.
         </p></dd><dt id="id-1.7.4.3.5.2.2.2.3"><span class="term"><code class="command">nodename=<em class="replaceable">NODE</em> state=resume</code></span></dt><dd><p>
          Marks the compute node as ready to return to the <code class="literal">idle</code>
          state.
         </p></dd><dt id="id-1.7.4.3.5.2.2.2.4"><span class="term"><code class="command">jobid=<em class="replaceable">JOBID</em>
         <em class="replaceable">REQUIREMENT</em>=<em class="replaceable">VALUE</em></code></span></dt><dd><p>
          Updates the given requirement, such as <code class="literal">NumNodes</code>,
          with a new value. This command can also be run as a non-privileged user.
         </p></dd></dl></div></dd><dt id="id-1.7.4.3.5.3"><span class="term"><code class="command">scontrol reconfigure</code></span></dt><dd><p>
       Triggers a reload of the configuration file
       <code class="filename">slurm.conf</code> on all compute nodes.
      </p></dd><dt id="id-1.7.4.3.5.4"><span class="term"><code class="command">scontrol reboot <em class="replaceable">NODELIST</em></code></span></dt><dd><p>
       Reboots a compute node, or group of compute nodes, when the jobs on
       it finish. To use this command, the option
       <code class="literal">RebootProgram="/sbin/reboot"</code> must be set in
       <code class="filename">slurm.conf</code>. When the reboot of a compute node takes
       more than 60 seconds, you can set a higher value in
       <code class="filename">slurm.conf</code>, such as <code class="literal">ResumeTimeout=300</code>.
      </p></dd></dl></div></section><section class="sect2" id="sec-slurm-sinfo" data-id-title="sinfo"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">sinfo</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-sinfo">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The command <code class="command">sinfo</code> retrieves information about the state
    of the compute nodes, and can be used for a fast overview of the cluster
    health. For more information, see <code class="command">man sinfo</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.4.3.1"><span class="term"><code class="command">--dead</code></span></dt><dd><p>
       Displays information about unresponsive nodes.
      </p></dd><dt id="id-1.7.4.4.3.2"><span class="term"><code class="command">--long</code></span></dt><dd><p>
       Shows more detailed information.
      </p></dd><dt id="id-1.7.4.4.3.3"><span class="term"><code class="command">--reservation</code></span></dt><dd><p>
       Prints information about advanced reservations.
      </p></dd><dt id="id-1.7.4.4.3.4"><span class="term"><code class="command">-R</code></span></dt><dd><p>
       Displays the reason a node is in the <code class="literal">down</code>,
       <code class="literal">drained</code>, or <code class="literal">failing</code> state.
      </p></dd><dt id="id-1.7.4.4.3.5"><span class="term"><code class="command">--state=<em class="replaceable">STATE</em></code></span></dt><dd><p>
       Limits the output only to nodes with the specified
       <em class="replaceable">STATE</em>.
      </p></dd></dl></div></section><section class="sect2" id="sec-slurm-sacctmgr-sacct" data-id-title="sacctmgr and sacct"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.3 </span><span class="title-name">sacctmgr and sacct</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-sacctmgr-sacct">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    These commands are used for managing accounting. For more information, see
    <code class="command">man sacctmgr</code> and <code class="command">man sacct</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.5.3.1"><span class="term"><code class="command">sacctmgr</code></span></dt><dd><p>
       Used for job accounting in Slurm. To use this command, the service
       <code class="literal">slurmdbd</code> must be set up. See
       <a class="xref" href="#sec-slurm-slurmdbd" title="5.1.2. Installing the Slurm database">Section 5.1.2, “Installing the Slurm database”</a>.
      </p></dd><dt id="id-1.7.4.5.3.2"><span class="term"><code class="command">sacct</code></span></dt><dd><p>
       Displays the accounting data if accounting is enabled.
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.5.3.2.2.2.1"><span class="term"><code class="command">--allusers</code></span></dt><dd><p>
          Shows accounting data for all users.
         </p></dd><dt id="id-1.7.4.5.3.2.2.2.2"><span class="term"><code class="command">--accounts</code>=<em class="replaceable">NAME</em></span></dt><dd><p>
          Shows only the specified user(s).
         </p></dd><dt id="id-1.7.4.5.3.2.2.2.3"><span class="term"><code class="command">--starttime</code>=<em class="replaceable">MM/DD[/YY]-HH:MM[:SS]</em></span></dt><dd><p>
          Shows only jobs after the specified start time. You can use just
          <em class="replaceable">MM/DD</em> or <em class="replaceable">HH:MM</em>.
          If no time is given, the command defaults to <code class="literal">00:00</code>,
          which means that only jobs from today are shown.
         </p></dd><dt id="id-1.7.4.5.3.2.2.2.4"><span class="term"><code class="command">--endtime</code>=<em class="replaceable">MM/DD[/YY]-HH:MM[:SS]</em></span></dt><dd><p>
          Accepts the same options as <code class="command">--starttime</code>. If no time
          is given, the time when the command was issued is used.
         </p></dd><dt id="id-1.7.4.5.3.2.2.2.5"><span class="term"><code class="command">--name</code>=<em class="replaceable">NAME</em></span></dt><dd><p>
          Limits output to jobs with the given <em class="replaceable">NAME</em>.
         </p></dd><dt id="id-1.7.4.5.3.2.2.2.6"><span class="term"><code class="command">--partition</code>=<em class="replaceable">PARTITION</em></span></dt><dd><p>
          Shows only jobs that run in the specified <em class="replaceable">PARTITION</em>.
         </p></dd></dl></div></dd></dl></div></section><section class="sect2" id="sec-slurm-sbatch-salloc-srun" data-id-title="sbatch, salloc, and srun"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.4 </span><span class="title-name">sbatch, salloc, and srun</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-sbatch-salloc-srun">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    These commands are used to schedule <span class="emphasis"><em>compute jobs</em></span>,
    which means batch scripts for the <code class="command">sbatch</code> command,
    interactive sessions for the <code class="command">salloc</code> command, or
    binaries for the <code class="command">srun</code> command. If the job cannot be
    scheduled immediately, only <code class="command">sbatch</code> places it into the queue.
   </p><p>
    For more information, see <code class="command">man sbatch</code>,
    <code class="command">man salloc</code>, and <code class="command">man srun</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.4.6.4.1"><span class="term"><code class="command">-n <em class="replaceable">COUNT_THREADS</em></code></span></dt><dd><p>
       Specifies the number of threads needed by the job. The threads can be
       allocated on different nodes.
      </p></dd><dt id="id-1.7.4.6.4.2"><span class="term"><code class="command">-N <em class="replaceable">MINCOUNT_NODES[-MAXCOUNT_NODES]</em></code></span></dt><dd><p>
       Sets the number of compute nodes required for a job. The
       <em class="replaceable">MAXCOUNT_NODES</em> number can be omitted.
      </p></dd><dt id="id-1.7.4.6.4.3"><span class="term"><code class="command">--time <em class="replaceable">TIME</em></code></span></dt><dd><p>
       Specifies the maximum clock time (runtime) after which a job is
       terminated. The format of <em class="replaceable">TIME</em> is either
       seconds or <em class="replaceable">[HH:]MM:SS</em>. Not to be confused
       with <code class="command">walltime</code>, which is <code class="literal">clocktime ×
       threads</code>.
      </p></dd><dt id="id-1.7.4.6.4.4"><span class="term"><code class="command">--signal <em class="replaceable">[B:]NUMBER[@TIME]</em></code></span></dt><dd><p>
       Sends the signal specified by <em class="replaceable">NUMBER</em>
       60 seconds before the end of the job, unless
       <em class="replaceable">TIME</em> is specified. The signal is
       sent to every process on every node. If a signal should only be sent
       to the controlling batch job, you must specify the
       <code class="command">B:</code> flag.
      </p></dd><dt id="id-1.7.4.6.4.5"><span class="term"><code class="command">--job-name <em class="replaceable">NAME</em></code></span></dt><dd><p>
       Sets the name of the job to <em class="replaceable">NAME</em> in the
       queue.
      </p></dd><dt id="id-1.7.4.6.4.6"><span class="term"><code class="command">--array=<em class="replaceable">RANGEINDEX</em></code></span></dt><dd><p>
       Executes the given script via <code class="command">sbatch</code> for indexes
       given by <em class="replaceable">RANGEINDEX</em> with the same
       parameters.
      </p></dd><dt id="id-1.7.4.6.4.7"><span class="term"><code class="command">--dependency=<em class="replaceable">STATE:JOBID</em></code></span></dt><dd><p>
       Defers the job until the specified <em class="replaceable">STATE</em> of
       the job <em class="replaceable">JOBID</em> is reached.
      </p></dd><dt id="id-1.7.4.6.4.8"><span class="term"><code class="command">--gres=<em class="replaceable">GRES</em></code></span></dt><dd><p>
       Runs a job only on nodes with the specified <span class="emphasis"><em>generic
       resource</em></span> (GRes), for example a GPU, specified by the value
       of <em class="replaceable">GRES</em>.
      </p></dd><dt id="id-1.7.4.6.4.9"><span class="term"><code class="command">--licenses=<em class="replaceable">NAME[:COUNT]</em></code></span></dt><dd><p>
       The job must have the specified number (<em class="replaceable">COUNT</em>)
       of licenses with the name <em class="replaceable">NAME</em>.
       A license is the opposite of a generic resource: it is not tied to a
       computer, but is a cluster-wide variable.
      </p></dd><dt id="id-1.7.4.6.4.10"><span class="term"><code class="command">--mem=<em class="replaceable">MEMORY</em></code></span></dt><dd><p>
       Sets the real <em class="replaceable">MEMORY</em> required by a
       job per node. To use this option, memory control must be enabled. The
       default unit for the <em class="replaceable">MEMORY</em> value is
       megabytes, but you can also use <code class="literal">K</code> for kilobyte,
       <code class="literal">M</code> for megabyte, <code class="literal">G</code> for gigabyte,
       or <code class="literal">T</code> for terabyte.
      </p></dd><dt id="id-1.7.4.6.4.11"><span class="term"><code class="command">--mem-per-cpu=<em class="replaceable">MEMORY</em></code></span></dt><dd><p>
       This option takes the same values as <code class="command">--mem</code>, but defines
       memory on a per-CPU basis rather than a per-node basis.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-slurm-upgrade" data-id-title="Upgrading Slurm"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Upgrading Slurm</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-upgrade">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For existing products under general support, version upgrades of Slurm are
    provided regularly. Unlike maintenance updates, these upgrades are not
    installed automatically using <code class="literal">zypper patch</code> but require
    you to request their installation explicitly. This ensures that these
    upgrades are not installed unintentionally and gives you the opportunity
    to plan version upgrades beforehand.
   </p><div id="id-1.7.5.3" data-id-title="zypper up is not recommended" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="command">zypper up</code> is not recommended</div><p>
       On systems running Slurm, updating packages with <code class="command">zypper up</code>
       is not recommended. <code class="command">zypper up</code> attempts to update all installed
       packages to the latest version, so might install a new major version of Slurm
       outside of planned Slurm upgrades.
     </p><p>
       Use <code class="command">zypper patch</code> instead, which only updates packages to the
       latest bug fix version.
     </p></div><section class="sect2" id="sec-slurm-upgrade-workflow" data-id-title="Slurm upgrade workflow"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Slurm upgrade workflow</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-upgrade-workflow">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Interoperability is guaranteed between three consecutive versions of Slurm,
    with the following restrictions:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      The version of <code class="literal">slurmdbd</code> must be identical to or higher
      than the version of <code class="literal">slurmctld</code>.
     </p></li><li class="listitem"><p>
      The version of <code class="literal">slurmctld</code> must the identical to or
      higher than the version of <code class="literal">slurmd</code>.
     </p></li><li class="listitem"><p>
      The version of <code class="literal">slurmd</code> must be identical to or higher
      than the version of the <code class="literal">slurm</code> user applications.
     </p></li></ol></div><p>
    Or in short:
    version(<code class="literal">slurmdbd</code>) &gt;=
    version(<code class="literal">slurmctld</code>) &gt;=
    version(<code class="literal">slurmd</code>) &gt;= version (Slurm user CLIs).
   </p><p>
    Slurm uses a segmented version number: the first two segments denote the
    major version, and the final segment denotes the patch level.
    Upgrade packages (that is, packages that were not initially supplied with
    the module or service pack) have their major version encoded in the package
    name (with periods <code class="literal">.</code> replaced by underscores
    <code class="literal">_</code>). For example, for version 18.08, this would be
    <code class="literal">slurm_18_08-*.rpm</code>.
   </p><p>
    With each version, configuration options for
    <code class="literal">slurmctld</code>, <code class="literal">slurmd</code>, or
    <code class="literal">slurmdbd</code> might be deprecated. While deprecated, they
    remain valid for this version and the two consecutive versions, but they might
    be removed later. Therefore, it is advisable to update the configuration files
    after the upgrade and replace deprecated configuration options before the
    final restart of a service.
   </p><p>
    A new major version of Slurm introduces a new version of
    <code class="literal">libslurm</code>. Older versions of this library might not work
    with an upgraded Slurm. An upgrade is provided for all SUSE Linux Enterprise software that
    depends on <code class="literal">libslurm </code>. It is strongly recommended to rebuild
    local applications using <code class="literal">libslurm</code>, such as MPI libraries
    with Slurm support, as early as possible. This might require updating the
    user applications, as new arguments might be introduced to existing functions.
   </p><div id="id-1.7.5.4.8" data-id-title="Upgrade slurmdbd databases before other Slurm components" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Upgrade <code class="literal">slurmdbd</code> databases before other Slurm components</div><p>
     If <code class="literal">slurmdbd</code> is used, always upgrade the
     <code class="literal">slurmdbd</code> database <span class="emphasis"><em>before</em></span> starting
     the upgrade of any other Slurm component. The same database can be connected
     to multiple clusters and must be upgraded before all of them.
    </p><p>
     Upgrading other Slurm components before the database can lead to data loss.
    </p></div></section><section class="sect2" id="sec-slurm-upgrade-database" data-id-title="Upgrading the slurmdbd database daemon"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Upgrading the <code class="literal">slurmdbd</code> database daemon</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-upgrade-database">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When upgrading <code class="literal">slurmdbd</code>,
    the database is converted when the new version of
    <code class="literal">slurmdbd</code> starts for the first time. If the
    database is big, the conversion could take several tens of minutes. During
    this time, the database is inaccessible.
   </p><p>
    It is highly recommended to create a backup of the database in case an
    error occurs during or after the upgrade process. Without a backup,
    all accounting data collected in the database might be lost if an error
    occurs or the upgrade is rolled back. A database
    converted to a newer version cannot be converted back to an older version,
    and older versions of <code class="literal">slurmdbd</code> do not recognize the
    newer formats.
   </p><div id="id-1.7.5.5.4" data-id-title="Convert primary slurmdbd first" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Convert primary <code class="systemitem">slurmdbd</code> first</div><p>
     If you are using a backup <code class="literal">slurmdbd</code>, the conversion must
     be performed on the primary <code class="literal">slurmdbd</code> first. The backup
     <code class="literal">slurmdbd</code> only starts after the conversion is complete.
    </p></div><div class="procedure" id="pro-slurm-upgrade-database" data-id-title="Upgrading the slurmdbd database daemon"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.5: </span><span class="title-name">Upgrading the <code class="literal">slurmdbd</code> database daemon </span></span><a title="Permalink" class="permalink" href="#pro-slurm-upgrade-database">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop the <code class="literal">slurmdbd</code> service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>rcslurmdbd stop</pre></div><p>
      Ensure that <code class="literal">slurmdbd</code> is not running anymore:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>rcslurmdbd status</pre></div><p>
      <code class="literal">slurmctld</code> might remain
      running while the database daemon is down. During this time, requests
      intended for <code class="literal">slurmdbd</code> are queued internally. The DBD
      Agent Queue size is limited, however, and should therefore be monitored
      with <code class="literal">sdiag</code>.
     </p></li><li class="step"><p>
      Create a backup of the <code class="literal">slurm_acct_db</code> database:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>mysqldump -p slurm_acct_db &gt; slurm_acct_db.sql</pre></div><p>
      If needed, this can be restored with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>mysql -p slurm_acct_db &lt; slurm_acct_db.sql</pre></div></li><li class="step"><p>
      During the database conversion, the variable <code class="literal">innodb_buffer_pool_size</code>
      must be set to a value of 128 MB or more. Check the current size:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>echo  'SELECT @@innodb_buffer_pool_size/1024/1024;' | \
mysql --password --batch</pre></div></li><li class="step"><p>
      If the value of <code class="literal">innodb_buffer_pool_size</code> is less than
      128 MB, you can change it for the duration of the current session
      (on <code class="literal">mariadb</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>echo 'set GLOBAL innodb_buffer_pool_size = 134217728;' | \
mysql --password --batch</pre></div><p>
      Alternatively, to permanently change the size, edit the <code class="filename">/etc/my.cnf</code>
      file, set <code class="literal">innodb_buffer_pool_size</code> to 128 MB,
      then restart the database:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>rcmysql restart</pre></div></li><li class="step"><p>
      If you need to update MariaDB, run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>zypper update mariadb</pre></div><p>
      Convert the database tables to the new version of MariaDB:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>mysql_upgrade --user=root --password=<em class="replaceable">ROOT_DB_PASSWORD</em>;</pre></div></li><li class="step"><p>
      Install the new version of <code class="literal">slurmdbd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>zypper install --force-resolution slurm_<em class="replaceable">VERSION</em>-slurmdbd</pre></div></li><li class="step"><p>
      Rebuild the database. If you are using a backup <code class="literal">slurmdbd</code>,
      perform this step on the primary <code class="literal">slurmdbd</code> first.
     </p><p>
      Because a conversion might take a considerable amount of time, the
      <code class="literal">systemd</code> service might time out during the conversion.
      Therefore, we recommend performing the migration manually by running
      <code class="literal">slurmdbd</code> from the command line in the foreground:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>/usr/sbin/slurmdbd -D -v</pre></div><p>
      When you see the following message, you can shut down
      <code class="literal">slurmdbd</code> by pressing
      <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>:
     </p><div class="verbatim-wrap"><pre class="screen">Conversion done:
success!</pre></div></li><li class="step"><p>
      Before restarting the service, remove or replace any deprecated
      configuration options. Check the deprecated options in the
      <a class="link" href="https://www.suse.com/releasenotes/x86_64/SLE-HPC/15-SP6" target="_blank"><em class="citetitle">Release Notes</em></a>.
     </p></li><li class="step"><p>
      Restart <code class="literal">slurmdbd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">DBnode # </code>systemctl start slurmdbd</pre></div><div id="id-1.7.5.5.5.10.3" data-id-title="No daemonization during rebuild" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: No daemonization during rebuild</div><p>
       During the rebuild of the Slurm database, the database daemon does not
       daemonize.
      </p></div></li></ol></div></div></section><section class="sect2" id="sec-slurm-upgrade-controller" data-id-title="Upgrading slurmctld and slurmd"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Upgrading <code class="literal">slurmctld</code> and <code class="literal">slurmd</code></span></span> <a title="Permalink" class="permalink" href="#sec-slurm-upgrade-controller">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After the Slurm database is upgraded, the <code class="literal">slurmctld</code> and
    <code class="literal">slurmd</code> instances can be upgraded. It is recommended to
    update the management servers and compute nodes all at once.
    If this is not feasible, the compute nodes (<code class="literal">slurmd</code>) can
    be updated on a node-by-node basis. However, the management servers
    (<code class="literal">slurmctld</code>) must be updated first.
   </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.7.5.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-slurm-upgrade-database" title="5.3.2. Upgrading the slurmdbd database daemon">Section 5.3.2, “Upgrading the <code class="literal">slurmdbd</code> database daemon”</a>. Upgrading other Slurm
      components before the database can lead to data loss.
     </p></li><li class="listitem"><p>
      This procedure assumes that MUNGE authentication is used and that
      <code class="literal">pdsh</code>, the <code class="literal">pdsh</code> Slurm plugin, and
      <code class="literal">mrsh</code> can access all of the machines in the cluster.
      If this is not the case, install <code class="literal">pdsh</code> by running
      <code class="command">zypper in pdsh-slurm</code>.
     </p><p>
      If <code class="literal">mrsh</code> is not used in the cluster, the
      <code class="literal">ssh</code> back-end for <code class="literal">pdsh</code> can be used
      instead. Replace the option <code class="literal">-R mrsh</code> with
      <code class="literal">-R ssh</code> in the <code class="literal">pdsh</code>commands below. This
      is less scalable and you might run out of usable ports.
     </p></li></ul></div><div class="procedure" id="pro-slurm-upgrade-controller" data-id-title="Upgrading slurmctld and slurmd"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.6: </span><span class="title-name">Upgrading <code class="literal">slurmctld</code> and <code class="literal">slurmd</code> </span></span><a title="Permalink" class="permalink" href="#pro-slurm-upgrade-controller">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Back up the configuration file<code class="filename">/etc/slurm/slurm.conf</code>.
      Because this file should be identical across the entire cluster, it is
      sufficient to do so only on the main management server.
     </p></li><li class="step"><p>
      On the main management server, edit <code class="filename">/etc/slurm/slurm.conf</code>
      and set <code class="literal">SlurmdTimeout</code> and <code class="literal">SlurmctldTimeout</code>
      to sufficiently high values to avoid timeouts while <code class="literal">slurmctld</code>
      and <code class="literal">slurmd</code> are down:
     </p><div class="verbatim-wrap"><pre class="screen">SlurmctldTimeout=3600
SlurmdTimeout=3600</pre></div><p>
      We recommend at least 60 minutes (<code class="literal">3600</code>), and more for
      larger clusters.
     </p></li><li class="step" id="st-copy-file-to-nodes"><p>
      Copy the updated <code class="filename">/etc/slurm/slurm.conf</code> from the
      management server to all nodes:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Obtain the list of partitions in
        <code class="filename">/etc/slurm/slurm.conf</code>.
       </p></li><li class="step"><p>
        Copy the updated configuration to the compute nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>cp /etc/slurm/slurm.conf /etc/slurm/slurm.conf.update
<code class="prompt root">management # </code>sudo -u slurm /bin/bash -c 'cat /etc/slurm/slurm.conf.update | \
pdsh -R mrsh -P <em class="replaceable">PARTITIONS</em> "cat &gt; /etc/slurm/slurm.conf"'
<code class="prompt root">management # </code>rm /etc/slurm/slurm.conf.update</pre></div></li><li class="step"><p>
        Reload the configuration file on all compute nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>scontrol reconfigure</pre></div></li><li class="step"><p>
        Verify that the reconfiguration took effect:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>scontrol show config | grep Timeout</pre></div></li></ol></li><li class="step"><p>
      Shut down all running <code class="literal">slurmctld</code> instances, first on
      any backup management servers, and then on the main management server:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl stop slurmctld</pre></div></li><li class="step"><p>
      Back up the <code class="literal">slurmctld</code> state files.
      <code class="literal">slurmctld</code> maintains persistent state information.
      Almost every major version involves changes to the <code class="literal">slurmctld</code>
      state files. This state information is upgraded if the upgrade remains
      within the supported version range and no data is lost.
     </p><p>
      However, if a downgrade is necessary, state information from
      newer versions is not recognized by an older version of
      <code class="literal">slurmctld</code> and is discarded, resulting in a
      loss of all running and pending jobs. Therefore, back up the
      old state in case an update needs to be rolled back.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Determine the <code class="literal">StateSaveLocation</code> directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>scontrol show config | grep StateSaveLocation</pre></div></li><li class="step"><p>
        Create a backup of the content of this directory. If a downgrade is
        required, restore the content of
        the <code class="literal">StateSaveLocation</code> directory from this backup.
       </p></li></ol></li><li class="step"><p>
      Shut down <code class="literal">slurmd</code> on the compute nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>pdsh -R ssh -P <em class="replaceable">PARTITIONS</em> systemctl stop slurmd</pre></div></li><li class="step"><p>
      Upgrade <code class="literal">slurmctld</code> on the main and backup management
      servers:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>zypper install --force-resolution slurm_<em class="replaceable">VERSION</em></pre></div><div id="id-1.7.5.6.4.8.3" data-id-title="Upgrade all Slurm packages at the same time" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Upgrade all Slurm packages at the same time</div><p>
       If any additional Slurm packages are installed, you must upgrade
       those as well. This includes:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          slurm-pam_slurm
         </p></li><li class="listitem"><p>
          slurm-sview
         </p></li><li class="listitem"><p>
          perl-slurm
         </p></li><li class="listitem"><p>
          slurm-lua
         </p></li><li class="listitem"><p>
          slurm-torque
         </p></li><li class="listitem"><p>
          slurm-config-man
         </p></li><li class="listitem"><p>
          slurm-doc
         </p></li><li class="listitem"><p>
          slurm-webdoc
         </p></li><li class="listitem"><p>
          slurm-auth-none
         </p></li><li class="listitem"><p>
          pdsh-slurm
         </p></li></ul></div><p>
        All Slurm packages must be upgraded at the same time to avoid conflicts
        between packages of different versions. This can be done by adding them to
        the <code class="literal">zypper install</code> command line described above.
      </p></div></li><li class="step"><p>
      Upgrade <code class="literal">slurmd</code> on the compute nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>pdsh -R ssh -P <em class="replaceable">PARTITIONS</em> \
zypper install --force-resolution slurm_<em class="replaceable">VERSION</em>-node</pre></div><div id="id-1.7.5.6.4.9.3" data-id-title="Memory size seen by slurmd might change on update" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Memory size seen by <code class="literal">slurmd</code> might change on update</div><p>
       Under certain circumstances, the amount of memory seen by
       <code class="literal">slurmd</code> might change after an update. If this happens,
       <code class="literal">slurmctld</code> puts the nodes in a
       <code class="literal">drained</code> state. To check whether the amount of
       memory seem by <code class="literal">slurmd</code> changed after the update,
       run the following command on a single compute node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">node1 # </code>slurmd -C</pre></div><p>
       Compare the output with the settings in
       <code class="filename">slurm.conf</code>. If required, correct the setting.
      </p></div></li><li class="step"><p>
      Before restarting the service, remove or replace any deprecated
      configuration options. Check the deprecated options in the
      <a class="link" href="https://www.suse.com/releasenotes/x86_64/SLE-HPC/15-SP6" target="_blank"><em class="citetitle">Release Notes</em></a>.
     </p><p>
      If you replace deprecated options in the configuration files, these
      configuration files can be distributed to all management servers and
      compute nodes in the cluster by using the method described in
      <a class="xref" href="#st-copy-file-to-nodes" title="Step 3">Step 3</a>.
     </p></li><li class="step"><p>
      Restart <code class="literal">slurmd</code> on all compute nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>pdsh -R ssh -P <em class="replaceable">PARTITIONS</em> systemctl start slurmd</pre></div></li><li class="step"><p>
      Restart <code class="literal">slurmctld</code> on the main and backup management servers:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl start slurmctld</pre></div></li><li class="step"><p>
      Check the status of the management servers. On the main and backup
      management servers, run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl status slurmctld</pre></div></li><li class="step"><p>
      Verify that the services are running without errors. Run the
      following command to check whether there are any <code class="literal">down</code>,
      <code class="literal">drained</code>, <code class="literal">failing</code>, or
      <code class="literal">failed</code> nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>sinfo -R</pre></div></li><li class="step"><p>
      Restore the original values of <code class="literal">SlurmdTimeout</code> and
      <code class="literal">SlurmctldTimeout</code> in <code class="literal">/etc/slurm/slurm.conf</code>,
      then copy the restored configuration to all nodes by using the method
      described in <a class="xref" href="#st-copy-file-to-nodes" title="Step 3">Step 3</a>.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-slurm-faq" data-id-title="Frequently asked questions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Frequently asked questions</span></span> <a title="Permalink" class="permalink" href="#sec-slurm-faq">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/slurm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="qandaset" id="id-1.7.6.2"><div class="free-id" id="id-1.7.6.2.1"> </div><dl class="qandaentry"><dt class="question" id="id-1.7.6.2.1.1"><strong>1.</strong>
       How do I change the state of a node from <code class="literal">down</code> to
       <code class="literal">up</code>?
      </dt><dd class="answer" id="id-1.7.6.2.1.2"><p>
       When the <code class="literal">slurmd</code> daemon on a node does not reboot in
       the time specified in the <code class="literal">ResumeTimeout</code> parameter, or
       the <code class="literal">ReturnToService</code> was not changed in the
       configuration file <code class="filename">slurm.conf</code>, compute nodes stay
       in the <code class="literal">down</code> state and must be set back to the
       <code class="literal">up</code> state manually. This can be done for the
       <em class="replaceable">NODE</em> with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>scontrol update state=resume NodeName=<em class="replaceable">NODE</em></pre></div></dd></dl><div class="free-id" id="id-1.7.6.2.2"> </div><dl class="qandaentry"><dt class="question" id="id-1.7.6.2.2.1"><strong>2.</strong>
       What is the difference between the states <code class="literal">down</code> and
       <code class="literal">down*</code>?
      </dt><dd class="answer" id="id-1.7.6.2.2.2"><p>
       A <code class="literal">*</code> shown after a status code means that the node is
       not responding.
      </p><p>
       When a node is marked as <code class="literal">down*</code>, it means that
       the node is not reachable because of network issues, or that
       <code class="literal">slurmd</code> is not running on that node.
      </p><p>
       In the <code class="literal">down</code> state, the node is reachable, but either
       the node was rebooted unexpectedly, the hardware does not match the
       description in <code class="filename">slurm.conf</code>, or a health check was
       configured with the <code class="literal">HealthCheckProgram</code>.
      </p></dd></dl><div class="free-id" id="id-1.7.6.2.3"> </div><dl class="qandaentry"><dt class="question" id="id-1.7.6.2.3.1"><strong>3.</strong>
       How do I get the exact core count, socket number, and number of CPUs for
       a node?
      </dt><dd class="answer" id="id-1.7.6.2.3.2"><p>
       To find the node values that go into the configuration file
       <code class="filename">slurm.conf</code>, run the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">node1 # </code>slurmd -C</pre></div></dd></dl></div></section></section><section xml:lang="en" class="chapter" id="cha-monitoring" data-id-title="Monitoring and logging"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Monitoring and logging</span></span> <a title="Permalink" class="permalink" href="#cha-monitoring">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Obtaining and maintaining an overview over the status and health
    of a cluster's compute nodes helps to ensure a smooth operation.
    This chapter describes tools that give an administrator an
    overview of the current cluster status, collect system
    logs, and gather information on certain system failure conditions.
   </p></div></div></div></div><section class="sect1" id="sec-remote-conman" data-id-title="ConMan — the console manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">ConMan — the console manager</span></span> <a title="Permalink" class="permalink" href="#sec-remote-conman">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   ConMan is a serial console management program designed to support many
   console devices and simultaneous users. It supports:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     local serial devices
    </p></li><li class="listitem"><p>
     remote terminal servers (via the telnet protocol)
    </p></li><li class="listitem"><p>
     IPMI Serial-Over-LAN (via FreeIPMI)
    </p></li><li class="listitem"><p>
     Unix domain sockets
    </p></li><li class="listitem"><p>
     external processes (for example, using <code class="command">expect</code> scripts
     for <code class="command">telnet</code>, <code class="command">ssh</code>, or
     <code class="command">ipmi-sol</code> connections)
    </p></li></ul></div><p>
   ConMan can be used for monitoring, logging, and optionally timestamping
   console device output.
  </p><p>
   To install ConMan, run <code class="command">zypper in conman</code>.
  </p><div id="id-1.8.3.6" data-id-title="conmand sends unencrypted data" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="systemitem">conmand</code> sends unencrypted data</div><p>
    The daemon <code class="systemitem">conmand</code> sends
    unencrypted data over the
    network and its connections are not authenticated. Therefore, it should
    be used locally only, listening to the port
    <code class="literal">localhost</code>. However, the IPMI console does offer
    encryption. This makes <code class="literal">conman</code> a good tool for
    monitoring many such consoles.
   </p></div><p>
   ConMan provides expect-scripts in the
   directory <code class="filename">/usr/lib/conman/exec</code>.
  </p><p>
   Input to <code class="literal">conman</code> is not echoed in interactive mode.
   This can be changed by entering the escape sequence
   <code class="literal">&amp;E</code>.
  </p><p>
   When pressing <span class="keycap">Enter</span> in interactive mode, no line
   feed is generated. To generate a line feed, press
   <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">L</span>.
  </p><p>
   For more information about options, see the ConMan man page.
  </p></section><section class="sect1" id="sec-monitoring-clusters-prometheus" data-id-title="Monitoring HPC clusters with Prometheus and Grafana"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Monitoring HPC clusters with Prometheus and Grafana</span></span> <a title="Permalink" class="permalink" href="#sec-monitoring-clusters-prometheus">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Monitor the performance of HPC clusters using Prometheus and Grafana.
  </p><p>
   Prometheus collects metrics from exporters running on cluster nodes
   and stores the data in a time series database. Grafana provides
   data visualization dashboards for the metrics collected by Prometheus.
   Preconfigured dashboards are available on the Grafana website.
  </p><p>
   The following Prometheus exporters are useful for High Performance Computing:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.8.4.5.1"><span class="term">Slurm exporter</span></dt><dd><p>
      Extracts job and job queue status metrics from the Slurm workload manager.
      Install this exporter on a node that has access to the Slurm
      command line interface.
     </p></dd><dt id="id-1.8.4.5.2"><span class="term">Node exporter</span></dt><dd><p>
      Extracts hardware and kernel performance metrics directly from each compute
      node. Install this exporter on every compute node you want to monitor.
     </p></dd></dl></div><div id="id-1.8.4.6" data-id-title="Restrict access to monitoring data" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Restrict access to monitoring data</div><p>
    It is recommended that the monitoring data only be accessible from
    within a trusted environment (for example, using a login node or VPN).
    It should not be accessible from the internet without additional security
    hardening measures for access restriction, access control, and encryption.
   </p></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">More information </span></span><a title="Permalink" class="permalink" href="#id-1.8.4.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Grafana:
     <a class="link" href="https://grafana.com/docs/grafana/latest/getting-started/" target="_blank">https://grafana.com/docs/grafana/latest/getting-started/</a>
    </p></li><li class="listitem"><p>
     Grafana dashboards:
     <a class="link" href="https://grafana.com/grafana/dashboards" target="_blank">https://grafana.com/grafana/dashboards</a>
    </p></li><li class="listitem"><p>
     Prometheus:
     <a class="link" href="https://prometheus.io/docs/introduction/overview/" target="_blank">https://prometheus.io/docs/introduction/overview/</a>
    </p></li><li class="listitem"><p>
     Prometheus exporters:
     <a class="link" href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank">https://prometheus.io/docs/instrumenting/exporters/</a>
    </p></li><li class="listitem"><p>
     Slurm exporter:
     <a class="link" href="https://github.com/vpenso/prometheus-slurm-exporter" target="_blank">https://github.com/vpenso/prometheus-slurm-exporter</a>
    </p></li><li class="listitem"><p>
     Node exporter:
     <a class="link" href="https://github.com/prometheus/node_exporter" target="_blank">https://github.com/prometheus/node_exporter</a>
    </p></li></ul></div><section class="sect2" id="sec-installing-prometheus-grafana" data-id-title="Installing Prometheus and Grafana"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2.1 </span><span class="title-name">Installing Prometheus and Grafana</span></span> <a title="Permalink" class="permalink" href="#sec-installing-prometheus-grafana">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Install Prometheus and Grafana on a management server, or on a
    separate monitoring node.
   </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.8.4.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
      You have an installation source for Prometheus and Grafana:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        The packages are available from SUSE Package Hub. To install SUSE Package Hub, see
        <a class="link" href="https://packagehub.suse.com/how-to-use/" target="_blank">https://packagehub.suse.com/how-to-use/</a>.
        
       </p></li><li class="listitem"><p>
        If you have a subscription for SUSE Manager, the packages are available
        from the SUSE Manager Client Tools repository.
       </p></li></ul></div></li></ul></div><div class="procedure" id="pro-installing-prometheus-grafana" data-id-title="Installing Prometheus and Grafana"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.1: </span><span class="title-name">Installing Prometheus and Grafana </span></span><a title="Permalink" class="permalink" href="#pro-installing-prometheus-grafana">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     In this procedure, replace <em class="replaceable">MNTRNODE</em> with the
     host name or IP address of the server where Prometheus and Grafana
     are installed.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Install the Prometheus and Grafana packages:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">monitor # </code>zypper in golang-github-prometheus-prometheus grafana</pre></div></li><li class="step"><p>
      Enable and start Prometheus:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">monitor # </code>systemctl enable --now prometheus</pre></div></li><li class="step"><p>
      Verify that Prometheus works:
     </p><ul class="stepalternatives">
      <li class="step"><p>
        In a browser, navigate to
        <code class="literal"><em class="replaceable">MNTRNODE</em>:9090/config</code>, or:
       </p></li>
      <li class="step"><p>
        In a terminal, run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>wget <em class="replaceable">MNTRNODE</em>:9090/config --output-document=-</pre></div></li>
     </ul><p>
      Either of these methods should show the default contents of the
      <code class="filename">/etc/prometheus/prometheus.yml</code> file.
     </p></li><li class="step"><p>
      Enable and start Grafana:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">monitor # </code>systemctl enable --now grafana-server</pre></div></li><li class="step"><p>
      Log in to the Grafana web server at
      <code class="literal"><em class="replaceable">MNTRNODE</em>:3000</code>.
     </p><p>
      Use <code class="literal">admin</code> for both the user name and password, then
      change the password when prompted.
     </p></li><li class="step"><p>
      On the left panel, select the gear icon (<span class="inlinemediaobject"><a href="images/grafana-configuration.png"><img src="images/grafana-configuration.png" width="13" alt="Gear icon, Configuration" title="Gear icon, Configuration"/></a></span>) and click <span class="guimenu">Data Sources</span>.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add data source</span>.
     </p></li><li class="step"><p>
      Find Prometheus and click <span class="guimenu">Select</span>.
     </p></li><li class="step"><p>
      In the <span class="guimenu">URL</span> field, enter <code class="literal">http://localhost:9090</code>.
      The default settings for the other fields can remain unchanged.
     </p><div id="id-1.8.4.8.4.11.2" class="admonition important compact"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><p>
       If Prometheus and Grafana are installed on different servers, replace
       <code class="literal">localhost</code> with the host name or IP address of the server
       where Prometheus is installed.
      </p></div></li><li class="step"><p>
      Click <span class="guimenu">Save &amp; Test</span>.
     </p></li></ol></div></div><p>
    You can now configure Prometheus to collect metrics from the cluster, and
    add dashboards to Grafana to visualize those metrics.
   </p></section><section class="sect2" id="sec-monitoring-cluster-workloads" data-id-title="Monitoring cluster workloads"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2.2 </span><span class="title-name">Monitoring cluster workloads</span></span> <a title="Permalink" class="permalink" href="#sec-monitoring-cluster-workloads">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To monitor the status of the nodes and jobs in an HPC cluster, install
    the Prometheus Slurm exporter to collect workload data, then import a
    custom Slurm dashboard from the Grafana website to visualize the data.
    For more information about this dashboard, see
    <a class="link" href="https://grafana.com/grafana/dashboards/4323" target="_blank">https://grafana.com/grafana/dashboards/4323</a>.
   </p><p>
    You must install the Slurm exporter on a node that has access to the
    Slurm command line interface. In the following procedure, the Slurm
    exporter will be installed on a management server.
   </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.8.4.9.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-installing-prometheus-grafana" title="6.2.1. Installing Prometheus and Grafana">Section 6.2.1, “Installing Prometheus and Grafana”</a> is complete.
     </p></li><li class="listitem"><p>
      The Slurm workload manager is fully configured.
     </p></li><li class="listitem"><p>
      You have internet access and policies that allow you to download the
      dashboard from the Grafana website.
     </p></li></ul></div><div class="procedure" id="pro-monitoring-cluster-workloads" data-id-title="Monitoring cluster workloads"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.2: </span><span class="title-name">Monitoring cluster workloads </span></span><a title="Permalink" class="permalink" href="#pro-monitoring-cluster-workloads">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     In this procedure, replace <em class="replaceable">MGMTSERVER</em> with the
     host name or IP address of the server where the Slurm exporter is installed,
     and replace <em class="replaceable">MNTRNODE</em> with the host name or IP
     address of the server where Grafana is installed.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Install the Slurm exporter:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>zypper in golang-github-vpenso-prometheus_slurm_exporter</pre></div></li><li class="step"><p>
      Enable and start the Slurm exporter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>systemctl enable --now prometheus-slurm_exporter</pre></div><div id="id-1.8.4.9.5.4.3" data-id-title="Slurm exporter fails when GPU monitoring is enabled" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Slurm exporter fails when GPU monitoring is enabled</div><p>
       In Slurm 20.11, the Slurm exporter fails when GPU monitoring is enabled.
      </p><p>
       This feature is disabled by default. Do not enable it for this version of Slurm.
      </p></div></li><li class="step"><p>
      Verify that the Slurm exporter works:
     </p><ul class="stepalternatives">
      <li class="step"><p>
        In a browser, navigate to
        <code class="literal"><em class="replaceable">MNGMTSERVER</em>:8080/metrics</code>, or:
       </p></li>
      <li class="step"><p>
        In a terminal, run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>wget <em class="replaceable">MGMTSERVER</em>:8080/metrics --output-document=-</pre></div></li>
     </ul><p>
      Either of these methods should show output similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"># HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 1.9521e-05
go_gc_duration_seconds{quantile="0.25"} 4.5717e-05
go_gc_duration_seconds{quantile="0.5"} 7.8573e-05
...</pre></div></li><li class="step"><p>
      On the server where Prometheus is installed, edit the
      <code class="literal">scrape_configs</code> section of the
      <code class="filename">/etc/prometheus/prometheus.yml</code> file to
      add a job for the Slurm exporter:
     </p><div class="verbatim-wrap"><pre class="screen">  - job_name: slurm-exporter
     scrape_interval: 30s
     scrape_timeout: 30s
     static_configs:
       - targets: ['<em class="replaceable">MGMTSERVER</em>:8080']</pre></div><p>
      Set the <code class="literal">scrape_interval</code> and <code class="literal">scrape_timeout</code>
      to <code class="literal">30s</code> to avoid overloading the server.
     </p></li><li class="step"><p>
      Restart the Prometheus service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">monitor # </code>systemctl restart prometheus</pre></div></li><li class="step"><p>
      Log in to the Grafana web server at
      <code class="literal"><em class="replaceable">MNTRNODE</em>:3000</code>.
     </p></li><li class="step"><p>
      On the left panel, select the plus icon (<span class="inlinemediaobject"><a href="images/grafana-create.png"><img src="images/grafana-create.png" width="13" alt="Plus icon, Create" title="Plus icon, Create"/></a></span>) and click <span class="guimenu">Import</span>.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Import via grafana.com</span> field, enter the dashboard
      ID <code class="literal">4323</code>, then click <span class="guimenu">Load</span>.
     </p></li><li class="step"><p>
      From the <span class="guimenu">Select a Prometheus data source</span> drop-down
      box, select the Prometheus data source added in
      <a class="xref" href="#pro-installing-prometheus-grafana" title="Installing Prometheus and Grafana">Procedure 6.1, “Installing Prometheus and Grafana”</a>, then click
      <span class="guimenu">Import</span>.
     </p></li><li class="step"><p>
      Review the Slurm dashboard. The data might take some time to appear.
     </p></li><li class="step"><p>
      If you made any changes, click <span class="guimenu">Save dashboard</span> when
      prompted, optionally describe your changes, then click <span class="guimenu">Save</span>.
     </p></li></ol></div></div><p>
    The Slurm dashboard is now available from the <span class="guimenu">Home</span>
    screen in Grafana.
   </p></section><section class="sect2" id="sec-monitoring-compute-node-performance" data-id-title="Monitoring compute node performance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2.3 </span><span class="title-name">Monitoring compute node performance</span></span> <a title="Permalink" class="permalink" href="#sec-monitoring-compute-node-performance">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To monitor the performance and health of each compute node,
    install the Prometheus node exporter to collect performance data, then
    import a custom node dashboard from the Grafana website to visualize
    the data. For more information about this dashboard, see
    <a class="link" href="https://grafana.com/grafana/dashboards/405" target="_blank">https://grafana.com/grafana/dashboards/405</a>.
   </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.8.4.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-installing-prometheus-grafana" title="6.2.1. Installing Prometheus and Grafana">Section 6.2.1, “Installing Prometheus and Grafana”</a> is complete.
     </p></li><li class="listitem"><p>
      You have internet access and policies that allow you to download the
      dashboard from the Grafana website.
     </p></li><li class="listitem"><p>
      To run commands on multiple nodes at once, <code class="command">pdsh</code>
      must be installed on the system your shell is running on, and
      SSH key authentication must be configured for all of the nodes.
      For more information, see <a class="xref" href="#sec-remote-pdsh" title="3.2. pdsh — parallel remote shell program">Section 3.2, “pdsh — parallel remote shell program”</a>.
     </p></li></ul></div><div class="procedure" id="pro-monitoring-compute-node-performance" data-id-title="Monitoring compute node performance"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.3: </span><span class="title-name">Monitoring compute node performance </span></span><a title="Permalink" class="permalink" href="#pro-monitoring-compute-node-performance">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     In this procedure, replace the example node names with the host names or IP
     addresses of the nodes, and replace <em class="replaceable">MNTRNODE</em> with
     the host name or IP address of the server where Grafana is installed.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Install the node exporter on each compute node. You can do this on multiple
      nodes at once by running the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>pdsh -R ssh -u root -w "<em class="replaceable">NODE1</em>,<em class="replaceable">NODE2</em>" \
"zypper in -y golang-github-prometheus-node_exporter"</pre></div></li><li class="step"><p>
      Enable and start the node exporter. You can do this on multiple nodes at
      once by running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">management # </code>pdsh -R ssh -u root -w "<em class="replaceable">NODE1</em>,<em class="replaceable">NODE2</em>" \
"systemctl enable --now prometheus-node_exporter"</pre></div></li><li class="step"><p>
      Verify that the node exporter works:
     </p><ul class="stepalternatives">
      <li class="step"><p>
        In a browser, navigate to
        <code class="literal"><em class="replaceable">NODE1</em>:9100/metrics</code>,
        or:
       </p></li>
      <li class="step"><p>
        In a terminal, run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>wget <em class="replaceable">NODE1</em>:9100/metrics --output-document=-</pre></div></li>
     </ul><p>
      Either of these methods should show output similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"># HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 2.3937e-05
go_gc_duration_seconds{quantile="0.25"} 3.5456e-05
go_gc_duration_seconds{quantile="0.5"} 8.1436e-05
...</pre></div></li><li class="step"><p>
      On the server where Prometheus is installed, edit the
      <code class="literal">scrape_configs</code> section of the
      <code class="filename">/etc/prometheus/prometheus.yml</code> file
      to add a job for the node exporter:
    </p><div class="verbatim-wrap"><pre class="screen">  - job_name: node-exporter
    static_configs:
      - targets: ['<em class="replaceable">NODE1</em>:9100']
      - targets: ['<em class="replaceable">NODE2</em>:9100']</pre></div><p>
      Add a target for every node that has the node exporter installed.
     </p></li><li class="step"><p>
      Restart the Prometheus service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">monitor # </code>systemctl restart prometheus</pre></div></li><li class="step"><p>
      Log in to the Grafana web server at
      <code class="literal"><em class="replaceable">MNTRNODE</em>:3000</code>.
     </p></li><li class="step"><p>
      On the left panel, select the plus icon (<span class="inlinemediaobject"><a href="images/grafana-create.png"><img src="images/grafana-create.png" width="13" alt="Plus icon, Create" title="Plus icon, Create"/></a></span>) and click <span class="guimenu">Import</span>.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Import via grafana.com</span> field, enter the dashboard
      ID <code class="literal">405</code>, then click <span class="guimenu">Load</span>.
     </p></li><li class="step"><p>
      From the <span class="guimenu">Select a Prometheus data source</span> drop-down
      box, select the Prometheus data source added in
      <a class="xref" href="#pro-installing-prometheus-grafana" title="Installing Prometheus and Grafana">Procedure 6.1, “Installing Prometheus and Grafana”</a>, then click
      <span class="guimenu">Import</span>.
     </p></li><li class="step"><p>
      Review the node dashboard. Click the <span class="guimenu">node</span>
      drop-down box to select the nodes you want to view. The data might take
      some time to appear.
     </p></li><li class="step"><p>
      If you made any changes, click <span class="guimenu">Save dashboard</span> when prompted.
      To keep the currently selected nodes next time you access the dashboard,
      activate <span class="guimenu">Save current variable values as dashboard default</span>.
      Optionally describe your changes, then click <span class="guimenu">Save</span>.
     </p></li></ol></div></div><p>
    The node dashboard is now available from the <span class="guimenu">Home</span> screen
    in Grafana.
   </p></section></section><section class="sect1" id="sec-monitoring-ras" data-id-title="rasdaemon — utility to log RAS error tracings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">rasdaemon — utility to log RAS error tracings</span></span> <a title="Permalink" class="permalink" href="#sec-monitoring-ras">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/monitoring.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">rasdaemon</code> is an RAS
   (Reliability, Availability and Serviceability) logging tool. It records
   memory errors using EDAC (Error Detection and Correction) tracing events.
   EDAC drivers in the Linux kernel handle detection of ECC (Error Correction
   Code) errors from memory controllers.
  </p><p>
   <code class="systemitem">rasdaemon</code> can be used on large
   memory systems to track, record, and localize memory errors and how they
   evolve over time to detect hardware degradation. Furthermore, it can be used
   to localize a faulty DIMM on the mainboard.
  </p><p>
   To check whether the EDAC drivers are loaded, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ras-mc-ctl --status</pre></div><p>
   The command should return <code class="literal">ras-mc-ctl: drivers are
   loaded</code>. If it indicates that the drivers are not loaded, EDAC
   may not be supported on your board.
  </p><p>
   To start <code class="systemitem">rasdaemon</code>, run
   <code class="command">systemctl start rasdaemon.service</code>.
   To start <code class="systemitem">rasdaemon</code>
   automatically at boot time, run <code class="command">systemctl enable
   rasdaemon.service</code>. The daemon logs information to
   <code class="filename">/var/log/messages</code> and to an internal database. A
   summary of the stored errors can be obtained with the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ras-mc-ctl --summary</pre></div><p>
   The errors stored in the database can be viewed with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ras-mc-ctl --errors</pre></div><p>
   Optionally, you can load the DIMM labels silk-screened on the system
   board to more easily identify the faulty DIMM. To do so, before starting
   <code class="systemitem">rasdaemon</code>, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl start ras-mc-ctl start</pre></div><p>
   For this to work, you need to set up a layout description for the board.
   There are no descriptions supplied by default. To add a layout
   description, create a file with an arbitrary name in the directory
   <code class="filename">/etc/ras/dimm_labels.d/</code>. The format is:
  </p><div class="verbatim-wrap"><pre class="screen">Vendor: <em class="replaceable">MOTHERBOARD-VENDOR-NAME</em>
Model: <em class="replaceable">MOTHERBOARD-MODEL-NAME</em>
  <em class="replaceable">LABEL</em>: <em class="replaceable">MC</em>.<em class="replaceable">TOP</em>.<em class="replaceable">MID</em>.<em class="replaceable">LOW</em></pre></div></section></section><section xml:lang="en" class="chapter" id="cha-compute" data-id-title="HPC user libraries"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">HPC user libraries</span></span> <a title="Permalink" class="permalink" href="#cha-compute">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Many HPC clusters need to accommodate multiple compute applications,
    each of which has its own very specific library dependencies. Multiple
    instances of the same libraries might exist, differing in version, build
    configuration, compiler, and MPI implementation. To manage these
    dependencies, you can use an environment module system. Most HPC
    libraries provided with the HPC module for SUSE Linux Enterprise Server are built with support for environment
    modules. This chapter describes the environment module system
    <span class="emphasis"><em>Lmod</em></span>, and a set of HPC
    compute libraries shipped with the HPC module.
   </p></div></div></div></div><section class="sect1" id="sec-compute-lmod" data-id-title="Lmod — Lua-based environment modules"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">Lmod — Lua-based environment modules</span></span> <a title="Permalink" class="permalink" href="#sec-compute-lmod">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Lmod is an advanced environment module system that allows the installation
   of multiple versions of a program or shared library, and helps configure the
   system environment for the use of a specific version. It supports
   hierarchical library dependencies and makes sure that the correct versions of
   dependent libraries are selected. Environment module-enabled library
   packages supplied with the HPC module support parallel installation of
   different versions and flavors of the same library or binary and are
   supplied with appropriate <code class="literal">lmod</code> module files.
  </p><section class="sect2" id="sec2-compute-lmod-basic" data-id-title="Installation and basic usage"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.1 </span><span class="title-name">Installation and basic usage</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-basic">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To install Lmod, run <code class="command">zypper in lua-lmod</code>.
   </p><p>
    Before you can use Lmod, you must <code class="command">source</code> an
    <code class="filename">init</code> file into the initialization file of your
    interactive shell. The following init files are available for various
    common shells:
   </p><div class="verbatim-wrap"><pre class="screen">/usr/share/lmod/lmod/init/bash
/usr/share/lmod/lmod/init/ksh
/usr/share/lmod/lmod/init/tcsh
/usr/share/lmod/lmod/init/zsh
/usr/share/lmod/lmod/init/sh</pre></div><p>
    Pick the appropriate file for your shell, then add the following line into
    your shell's init file:
   </p><div class="verbatim-wrap"><pre class="screen">source /usr/share/lmod/lmod/init/<em class="replaceable">INIT-FILE</em></pre></div><p>
    The init script adds the command <code class="command">module</code>.
   </p></section><section class="sect2" id="sec2-compute-lmod-lista" data-id-title="Listing available modules"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.2 </span><span class="title-name">Listing available modules</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-lista">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To list all the available modules, run <code class="command">module spider</code>.
    To show all modules which can be loaded with the currently loaded modules,
    run <code class="command">module avail</code>. A module name consists of a name and
    a version string, separated by a <code class="literal">/</code> character. If more
    than one version is available for a certain module name, the default
    version is marked by a <code class="literal">*</code> character. If there is no default,
    the module with the highest version number is loaded. To reference a specific module
    version, you can use the full string
    <code class="literal"><em class="replaceable">NAME</em>/<em class="replaceable">VERSION</em></code>.
   </p></section><section class="sect2" id="sec2-compute-lmod-listl" data-id-title="Listing loaded modules"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.3 </span><span class="title-name">Listing loaded modules</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-listl">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <code class="command">module list</code> shows all currently loaded modules. Refer to
    <code class="command">module help</code> for some short help on the module command,
    and <code class="command">module help <em class="replaceable">MODULE-NAME</em></code>
    for help on the particular module. The <code class="command">module</code> command is
    only available when you log in after installing
    <span class="package">lua-lmod</span>.
   </p></section><section class="sect2" id="sec2-compute-lmod-info" data-id-title="Gathering information about a module"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.4 </span><span class="title-name">Gathering information about a module</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-info">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To get information about a particular module, run <code class="command">module whatis
    <em class="replaceable">MODULE-NAME</em></code>. To load a module, run
    <code class="command">module load <em class="replaceable">MODULE-NAME</em></code>. This
    will ensure that your environment is modified (that is, the
    <code class="literal">PATH</code> and <code class="literal">LD_LIBRARY_PATH</code> and other
    environment variables are prepended) so that binaries and libraries
    provided by the respective modules are found. To run a program compiled
    against this library, the appropriate <code class="command">module load</code>
    commands must be issued beforehand.
   </p></section><section class="sect2" id="sec2-compute-lmod-load" data-id-title="Loading modules"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.5 </span><span class="title-name">Loading modules</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-load">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">module load <em class="replaceable">MODULE</em></code>
    command must be run in the shell from which the module is to be used.
    Some modules require a compiler toolchain or MPI flavor module to be loaded
    before they are available for loading.
   </p></section><section class="sect2" id="sec2-compute-lmod-env" data-id-title="Environment variables"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.6 </span><span class="title-name">Environment variables</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-env">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If the respective development packages are installed, build-time
    environment variables like <code class="literal">LIBRARY_PATH</code>,
    <code class="literal">CPATH</code>, <code class="literal">C_INCLUDE_PATH</code>, and
    <code class="literal">CPLUS_INCLUDE_PATH</code> are set up to include the
    directories containing the appropriate header and library files. However,
    some compiler and linker commands might not honor these. In this case, use
    the appropriate options together with the environment variables <code class="literal">-I
    <em class="replaceable">PACKAGE_NAME</em>_INC</code> and <code class="literal">-L
    <em class="replaceable">PACKAGE_NAME</em>_LIB</code> to add the include
    and library paths to the command lines of the compiler and linker.
   </p></section><section class="sect2" id="sec2-compute-lmod-moreinfo" data-id-title="For more information"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.7 </span><span class="title-name">For more information</span></span> <a title="Permalink" class="permalink" href="#sec2-compute-lmod-moreinfo">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For more information on Lmod, see
    <a class="link" href="https://lmod.readthedocs.org" target="_blank">https://lmod.readthedocs.org</a>.
   </p></section></section><section class="sect1" id="sec-compiler" data-id-title="GNU Compiler Toolchain Collection for HPC"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">GNU Compiler Toolchain Collection for HPC</span></span> <a title="Permalink" class="permalink" href="#sec-compiler">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the HPC module for SUSE Linux Enterprise Server, the GNU compiler collection version 7 is provided as the base
   compiler toolchain. The <span class="package">gnu-compilers-hpc</span> package provides the
   environment module for the base version of the GNU compiler suite. This
   package must be installed when using any of the HPC libraries enabled for
   environment modules.
  </p><section class="sect2" id="sec-environment-module" data-id-title="Environment module"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.1 </span><span class="title-name">Environment module</span></span> <a title="Permalink" class="permalink" href="#sec-environment-module">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This package requires <span class="package">lua-lmod</span> to supply environment
    module support.
   </p><p>
    To install <span class="package">gnu-compilers-hpc</span>, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>sudo zypper in gnu-compilers-hpc</pre></div><p>
    To make libraries built with the base compilers available, you must set
    up the environment appropriately and select the GNU toolchain. To do so,
    run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load gnu</pre></div></section><section class="sect2" id="sec-building-software-with-compiler" data-id-title="Building High Performance Computing software with GNU Compiler Suite"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.2 </span><span class="title-name">Building High Performance Computing software with GNU Compiler Suite</span></span> <a title="Permalink" class="permalink" href="#sec-building-software-with-compiler">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To use the GNU compiler collection to build your own libraries and
    applications, <span class="package">gnu-compilers-hpc-devel</span> must be
    installed. It ensures that all compiler components required for HPC (that is,
    C, C++, and Fortran compilers) are installed.
   </p><p>
    The environment variables <code class="literal">CC</code>, <code class="literal">CXX</code>,
    <code class="literal">FC</code> and <code class="literal">F77</code> will be set correctly and
    the path will be adjusted so that the correct compiler version can be
    found.
   </p></section><section class="sect2" id="sec-compiler-later-versions" data-id-title="Later versions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.3 </span><span class="title-name">Later versions</span></span> <a title="Permalink" class="permalink" href="#sec-compiler-later-versions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The Development Tools module might provide later versions of the GNU compiler
    suite. To determine the available compiler suites, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>zypper search '*-compilers-hpc'</pre></div><p>
    If you have more than one version of the compiler suite installed,
    <span class="emphasis"><em>Lmod</em></span> picks the latest one by default. If you
    require an older version, or the base version, append the version
    number:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load gnu/7</pre></div><p>
    For more information, see <a class="xref" href="#sec-compute-lmod" title="7.1. Lmod — Lua-based environment modules">Section 7.1, “Lmod — Lua-based environment modules”</a>.
   </p></section></section><section class="sect1" id="sec-compute-lib" data-id-title="High Performance Computing libraries"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">High Performance Computing libraries</span></span> <a title="Permalink" class="permalink" href="#sec-compute-lib">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Library packages that support environment modules follow a distinctive
   naming scheme. All packages have the compiler suite and, if built with MPI
   support, the MPI flavor included in their name:
   <code class="literal">*-[<em class="replaceable">MPI_FLAVOR</em>-]<em class="replaceable">COMPILER</em>-hpc*</code>.
   To allow the parallel installation of multiple versions of a library,
   the package name contains the version number (with dots <code class="literal">.</code>
   replaced by underscores <code class="literal">_</code>). <span class="package">master-</span>
   packages are supplied to
   ensure that the latest version of a package is installed. When these master
   packages are updated, the latest version of the respective packages is
   installed, while leaving previous versions installed. Library packages are
   split between runtime and compile-time packages. The compile-time packages
   typically supply <code class="literal">include</code> files and <code class="literal">.so</code>
   files for shared libraries. Compile-time package names end with
   <code class="literal">-devel</code>. For some libraries, static
   (<code class="literal">.a</code>) libraries are supplied as well. Package names for
   these end with <code class="literal">-devel-static</code>.
  </p><p>
   As an example, these are the package names of the ADIOS library version 1.13.1,
   built with GCC for Open MPI v4:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     library master package: <span class="package">adios-gnu-openmpi4-hpc</span>
    </p></li><li class="listitem"><p>
     development master package: <span class="package">adios-gnu-openmpi4-hpc-devel</span>
    </p></li><li class="listitem"><p>
     library package: <span class="package">adios_1_13_1-gnu-openmpi4-hpc</span>
    </p></li><li class="listitem"><p>
     development package: <span class="package">adios_1_13_1-gnu-openmpi4-hpc-devel</span>
    </p></li><li class="listitem"><p>
     static library package: <span class="package">adios_1_13_1-gnu-openmpi4-hpc-devel-static</span>
    </p></li></ul></div><p>
   To install a library package, run <code class="command">zypper in
   <em class="replaceable">LIBRARY-MASTER-PACKAGE</em></code>. To install a
   development file, run <code class="command">zypper in
   <em class="replaceable">LIBRARY-DEVEL-MASTER-PACKAGE</em></code>.
  </p><p>
   The GNU compiler collection version 7 as provided with the HPC module
   and the MPI flavors Open MPI v.3, Open MPI v.4, MPICH, and MVAPICH2 are
   currently supported.
  </p><p>
   The Development Tools module might provide later versions of the GNU compiler
   suite. To view available compilers, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>zypper search '*-compilers-hpc'</pre></div><section class="sect2" id="sec2-lib-numpy" data-id-title="NumPy Python library"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.1 </span><span class="title-name">NumPy Python library</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-numpy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    NumPy is a general-purpose array-processing package designed to efficiently
    manipulate large multi-dimensional arrays of arbitrary records without
    sacrificing too much speed for small multi-dimensional arrays.
   </p><p>
    NumPy is built on the Numeric code base and adds features introduced by the
    discontinued <span class="emphasis"><em>NumArray</em></span> project, as well as an extended
    C API, and the ability to create arrays of arbitrary type, which also makes
    NumPy suitable for interfacing with general-purpose database applications.
   </p><p>
    There are also basic facilities for discrete Fourier transform, basic
    linear algebra, and random number generation.
   </p><p>
    This package is available both for Python 2 and 3. The specific compiler
    toolchain module must be loaded for this library. The correct library
    module for the Python version used needs to be specified when loading this
    module. To load this module, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> python<em class="replaceable">VERSION</em>-numpy</pre></div><p>
    For information about the toolchain to load see: <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">python<em class="replaceable">VERSION</em>-numpy-gnu-hpc</code>
     </p></li><li class="listitem"><p>
      <code class="literal">python<em class="replaceable">VERSION</em>-numpy-gnu-hpc-devel</code>
     </p></li></ul></div></section><section class="sect2" id="sec2-lib-scipy" data-id-title="SciPy Python Library"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.2 </span><span class="title-name">SciPy Python Library</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-scipy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SciPy is a collection of mathematical algorithms and convenience functions
    built on the NumPy extension of Python. It provides high-level commands
    and classes for manipulating and visualizing data. With SciPy, an
    interactive Python session becomes a data-processing and system-prototyping
    environment.
   </p><p>
    This package is available both for Python 2 (up to version 1.2.0 only) and
    3. The specific compiler toolchain modules must be loaded for this library.
    The correct library module for the Python version used must be
    specified when loading this module. To load this module, run the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> python<em class="replaceable">VERSION</em>-scipy</pre></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">python<em class="replaceable">PYTHON_VERSION</em>-scipy-gnu-hpc</code>
     </p></li><li class="listitem"><p>
      <code class="literal">python<em class="replaceable">PYTHON_VERSION</em>-scipy-gnu-hpc-devel</code>
     </p></li></ul></div></section><section class="sect2" id="sec-remote-memkind" data-id-title="memkind — heap manager for heterogeneous memory platforms and mixed memory policies"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.3 </span><span class="title-name">memkind — heap manager for heterogeneous memory platforms and mixed memory policies</span></span> <a title="Permalink" class="permalink" href="#sec-remote-memkind">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <span class="emphasis"><em>memkind</em></span> library is a user-extensible heap manager
    built on top of <span class="emphasis"><em>jemalloc</em></span>. It enables control over
    memory characteristics and a partitioning of the heap between kinds of
    memory. The kinds of memory are defined by operating system memory policies
    that have been applied to virtual address ranges. Memory characteristics
    supported by <span class="package">memkind</span> without user extension include
    control of NUMA and page size features.
   </p><p>
    For more information, see:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      the man pages <code class="literal">memkind</code> and <code class="literal">hbwallow</code>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://github.com/memkind/memkind" target="_blank">https://github.com/memkind/memkind</a>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://memkind.github.io/memkind/" target="_blank">https://memkind.github.io/memkind/</a>
     </p></li></ul></div><div id="id-1.9.5.11.5" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     This tool is only available for AMD64/Intel 64.
    </p></div></section><section class="sect2" id="sec2-lib-pmix" data-id-title="Support for PMIx in Slurm and MPI libraries"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.4 </span><span class="title-name">Support for PMIx in Slurm and MPI libraries</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-pmix">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    PMIx abstracts the internals of MPI implementations for workload managers
    and unifies the way MPI jobs are started by the workload manager. With
    PMIx, there is no need to use the individual MPI launchers on Slurm,
    because <code class="command">srun</code> will take care of this. In addition, the
    workload manager can determine the topology of the cluster, so you do not
    need to specify topologies manually.
   </p></section><section class="sect2" id="sec2-lib-blas" data-id-title="OpenBLAS library — optimized BLAS library"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.5 </span><span class="title-name">OpenBLAS library — optimized BLAS library</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-blas">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    OpenBLAS is an optimized BLAS (Basic Linear Algebra Subprograms) library
    based on GotoBLAS2 1.3, BSD version. It provides the BLAS API. It is
    shipped as a package enabled for environment modules, so it requires
    using Lmod to select a version. There are two variants of this library: an
    OpenMP-enabled variant, and a <code class="literal">pthreads</code> variant.
   </p><div class="sect5 bridgehead"><h6 class="title" id="id-1.9.5.13.3"><span class="name">OpenMP-Enabled Variant</span><a title="Permalink" class="permalink" href="#id-1.9.5.13.3">#</a></h6></div><p>
    The OpenMP variant covers the following use cases:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="bold"><strong>Programs using OpenMP.</strong></span> This requires the
      OpenMP-enabled library version to function correctly.
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Programs using pthreads.</strong></span> This requires
      an OpenBLAS library without pthread support. This can be achieved with
      the OpenMP-version. We recommend limiting the number of threads that are
      used to 1 by setting the environment variable
      <code class="literal">OMP_NUM_THREADS=1</code>.
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Programs without pthreads and without
      OpenMP.</strong></span> Such programs can still take advantage of the OpenMP
      optimization in the library by linking against the OpenMP variant of the
      library.
     </p></li></ul></div><p>
    When linking statically, ensure that <code class="literal">libgomp.a</code> is
    included by adding the linker flag <code class="literal">-lgomp</code>.
   </p><div class="sect5 bridgehead"><h6 class="title" id="id-1.9.5.13.7"><span class="name">pthreads Variant</span><a title="Permalink" class="permalink" href="#id-1.9.5.13.7">#</a></h6></div><p>
    The pthreads variant of the OpenBLAS library can improve the performance of
    single-threaded programs. The number of threads used can be controlled with
    the environment variable <code class="literal">OPENBLAS_NUM_THREADS</code>.
   </p><div class="sect5 bridgehead"><h6 class="title" id="id-1.9.5.13.9"><span class="name">Installation and Usage</span><a title="Permalink" class="permalink" href="#id-1.9.5.13.9">#</a></h6></div><p>
    This module requires loading a compiler toolchain beforehand. To select the
    latest version of this module provided, run the following command:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Standard version:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> openblas</pre></div></li><li class="listitem"><p>
      OpenMP/pthreads version:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> openblas-pthreads</pre></div></li></ul></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">libopenblas-gnu-hpc</code>
     </p></li><li class="listitem"><p>
      <code class="literal">libopenblas-gnu-hpc-devel</code>
     </p></li><li class="listitem"><p>
      <code class="literal">libopenblas-pthreads-gnu-hpc</code>
     </p></li><li class="listitem"><p>
      <code class="literal">libopenblas-pthreads-gnu-hpc-devel</code>
     </p></li></ul></div></section></section><section class="sect1" id="FileFormat" data-id-title="File format libraries"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">File format libraries</span></span> <a title="Permalink" class="permalink" href="#FileFormat">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec2-lib-hdf5" data-id-title="HDF5 HPC library — model, library, and file format for storing and managing data"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.1 </span><span class="title-name">HDF5 HPC library — model, library, and file format for storing and managing data</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-hdf5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    HDF5 is a data model, library, and file format for storing and managing
    data. It supports an unlimited variety of data types, and is designed for
    flexible and efficient I/O and for high-volume and complex data. HDF5 is
    portable and extensible, allowing applications to evolve in their use of
    HDF5.
   </p><p>
    There are serial and MPI variants of this library available. All flavors
    require loading a compiler toolchain module beforehand. The MPI variants
    also require loading the correct MPI flavor module.
   </p><p>
    To load the highest available serial version of this module, run the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> hdf5</pre></div><p>
    When an MPI flavor is loaded, you can load the MPI version of this module
    by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> <em class="replaceable">MPI_FLAVOR</em> phdf5</pre></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>. For information about available MPI flavors, see
    <a class="xref" href="#sec1-MPI-libs" title="7.5. MPI libraries">Section 7.5, “MPI libraries”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="package">hdf5-hpc-examples</span>
     </p></li><li class="listitem"><p>
      <span class="package">hdf5-gnu-hpc-devel</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5-gnu-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_cpp-gnu-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_fortran-gnu-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_hl_cpp-gnu-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_hl_fortran-gnu-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">hdf5-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc-devel</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_fortran-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">libhdf5_hl_fortran-<em class="replaceable">MPI_FLAVOR</em>-hpc</span>
     </p></li></ul></div><p>
    <em class="replaceable">MPI_FLAVOR</em> must be one of the supported MPI
    flavors described in <a class="xref" href="#sec1-MPI-libs" title="7.5. MPI libraries">Section 7.5, “MPI libraries”</a>.
   </p><p>
    For general information about Lmod and modules, see
    <a class="xref" href="#sec-compute-lmod" title="7.1. Lmod — Lua-based environment modules">Section 7.1, “Lmod — Lua-based environment modules”</a>.
   </p></section></section><section class="sect1" id="sec1-MPI-libs" data-id-title="MPI libraries"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">MPI libraries</span></span> <a title="Permalink" class="permalink" href="#sec1-MPI-libs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Three different implementation of the Message Passing Interface (MPI)
   standard are provided standard with the HPC module:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Open MPI (version 3 and version 4)
    </p></li><li class="listitem"><p>
     MVAPICH2
    </p></li><li class="listitem"><p>
     MPICH
    </p></li></ul></div><p>
   These packages have been built with full environment module support (LMOD).
  </p><p>
   The following packages are available:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For Open MPI:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       user programs: <code class="literal">openmpi3-gnu-hpc</code> and
       <code class="literal">openmpi4-gnu-hpc</code>
      </p></li><li class="listitem"><p>
       shared libraries: <code class="literal">libopenmpi3-gnu-hpc</code> and
       <code class="literal">libopenmpi4-gnu-hpc</code>
      </p></li><li class="listitem"><p>
       development libraries, headers and tools required for building:
       <code class="literal">openmpi3-gnu-hpc-devel</code> and
       <code class="literal">openmpi4-gnu-hpc-devel</code>
      </p></li><li class="listitem"><p>
       documentation: <code class="literal">openmpi3-gnu-hpc-docs</code> and
       <code class="literal">openmpi4-gnu-hpc-docs</code>.
      </p></li></ul></div></li><li class="listitem"><p>
     For MVAPICH2
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       user programs and libraries: <code class="literal">mvapich2-gnu-hpc</code>
      </p></li><li class="listitem"><p>
       development libraries, headers and tools for building:
       <code class="literal">mvapich2-gnu-hpc-devel</code>
      </p></li><li class="listitem"><p>
       documentation: <code class="literal">mvapich2-gnu-hpc-doc</code>
      </p></li></ul></div><p>
     For MPICH:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       user programs and libraries: <code class="literal">mpich-gnu-hpc</code>
      </p></li><li class="listitem"><p>
       development libraries, headers and tools for building:
       <code class="literal">mpich-gnu-hpc-devel</code>
      </p></li></ul></div></li></ul></div><p>
   The different MPI implementations and versions are independent of each other,
   and can be installed in parallel.
  </p><p>
   Use environment modules to pick the version to use:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For Open MPI v.3:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> openmpi/3</pre></div></li><li class="listitem"><p>
     For Open MPI v.4:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> openmpi/4</pre></div></li><li class="listitem"><p>
     For MVAPICH2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> mvapich2</pre></div></li><li class="listitem"><p>
     For MPICH:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> mpich</pre></div></li></ul></div><p>
   For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>.
  </p></section><section class="sect1" id="sec1-packages-profiling-benchmark" data-id-title="Profiling and benchmarking libraries and tools"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Profiling and benchmarking libraries and tools</span></span> <a title="Permalink" class="permalink" href="#sec1-packages-profiling-benchmark">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The HPC module for SUSE Linux Enterprise Server provides tools for profiling MPI applications and benchmarking
   MPI performance.
  </p><section class="sect2" id="sec2-tool-imb" data-id-title="IMB — Intel* MPI benchmarks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6.1 </span><span class="title-name">IMB — Intel* MPI benchmarks</span></span> <a title="Permalink" class="permalink" href="#sec2-tool-imb">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The Intel* MPI Benchmarks package provides a set of elementary
    benchmarks that conform to the MPI-1, MPI-2, and MPI-3 standards. You can
    run all of the supported benchmarks, or a subset specified in the command
    line, using a single executable file. Use command line parameters to
    specify various settings, such as time measurement, message lengths, and
    selection of communicators. For details, see the Intel* MPI
    Benchmarks User's Guide:
    <a class="link" href="https://software.intel.com/en-us/imb-user-guide" target="_blank">https://software.intel.com/en-us/imb-user-guide</a>.
   </p><p>
    For the IMB binaries to be found, a compiler toolchain and an MPI flavor
    must be loaded beforehand. To load this module, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> <em class="replaceable">MPI_FLAVOR</em> imb</pre></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>. For information on available MPI
    flavors, see <a class="xref" href="#sec1-MPI-libs" title="7.5. MPI libraries">Section 7.5, “MPI libraries”</a>.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">imb-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc</code>
     </p></li></ul></div></section><section class="sect2" id="sec2-lib-papi" data-id-title="PAPI HPC library — consistent interface for hardware performance counters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6.2 </span><span class="title-name">PAPI HPC library — consistent interface for hardware performance counters</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-papi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    PAPI provides a tool with a consistent
    interface and methodology for the performance counter hardware found
    in most major microprocessors.
   </p><p>
    This package works with all compiler toolchains and does not require a
    compiler toolchain to be selected. Load the latest version provided by running
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> papi</pre></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="package">papi-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">papi-hpc-devel</span>
     </p></li></ul></div><p>
    For general information about Lmod and modules, see <a class="xref" href="#sec-compute-lmod" title="7.1. Lmod — Lua-based environment modules">Section 7.1, “Lmod — Lua-based environment modules”</a>.
   </p></section><section class="sect2" id="sec2-lib-mpip" data-id-title="mpiP — lightweight MPI profiling library"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6.3 </span><span class="title-name">mpiP — lightweight MPI profiling library</span></span> <a title="Permalink" class="permalink" href="#sec2-lib-mpip">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    mpiP is a lightweight profiling library for MPI applications. Because it
    only collects statistical information about MPI functions, mpiP generates
    considerably less overhead and much less data than tracing tools. All the
    information captured by mpiP is task-local. It only uses communication
    during report generation, typically at the end of the experiment, to merge
    results from all of the tasks into one output file.
   </p><p>
    For this library a compiler toolchain and MPI flavor must be loaded
    beforehand. To load this module, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>module load <em class="replaceable">TOOLCHAIN</em> <em class="replaceable">MPI_FLAVOR</em> mpip</pre></div><p>
    For information about the toolchain to load, see <a class="xref" href="#sec-compiler" title="7.2. GNU Compiler Toolchain Collection for HPC">Section 7.2, “GNU Compiler Toolchain Collection for HPC”</a>. For information on available MPI
    flavors, see <a class="xref" href="#sec1-MPI-libs" title="7.5. MPI libraries">Section 7.5, “MPI libraries”</a>.
   </p><p>
    List of master packages:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="package">mpiP-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc</span>
     </p></li><li class="listitem"><p>
      <span class="package">mpiP-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc-devel</span>
     </p></li><li class="listitem"><p>
      <code class="literal">mpiP-gnu-<em class="replaceable">MPI_FLAVOR</em>-hpc-doc</code>
     </p></li></ul></div><p>
    <em class="replaceable">MPI_FLAVOR</em> must be one of the supported MPI
    flavors described in <a class="xref" href="#sec1-MPI-libs" title="7.5. MPI libraries">Section 7.5, “MPI libraries”</a>.
   </p></section></section><section xml:lang="en" class="sect1" id="sec-environment-containers" data-id-title="Creating environment containers with Apptainer"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.7 </span><span class="title-name">Creating environment containers with Apptainer</span></span> <a title="Permalink" class="permalink" href="#sec-environment-containers">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/environment-containers.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can deploy environments with preconfigured environment variables by using
    <span class="emphasis"><em>environment containers</em></span>. Environment containers include only the
    components that are part of the environment, plus any required user applications.
    To create a container from the current HPC environment, use the container platform
    Apptainer (formerly called Singularity). Apptainer is available from SUSE Package Hub.
    You can also use Spack to configure the environment to use with Apptainer.
  </p><p>
    For more information, see the following documentation:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Enabling the SUSE Package Hub extension: <a class="link" href="https://packagehub.suse.com/how-to-use/" target="_blank">https://packagehub.suse.com/how-to-use/</a>.
      </p></li><li class="listitem"><p>
        Using Spack to configure the environment:
        <a class="link" href="https://spack.readthedocs.io/en/latest/containers.html#" target="_blank">https://spack.readthedocs.io/en/latest/containers.html#</a>.
      </p></li><li class="listitem"><p>
        Apptainer documentation: <a class="link" href="https://apptainer.org/docs/" target="_blank">https://apptainer.org/docs/</a>.
      </p></li></ul></div><p>
    To migrate from Singularity to Apptainer, see
    <a class="link" href="https://apptainer.org/docs/admin/latest/singularity_migration.html" target="_blank">https://apptainer.org/docs/admin/latest/singularity_migration.html</a>.
  </p></section></section><section xml:lang="en" class="chapter" id="cha-spack" data-id-title="Spack package management tool"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Spack package management tool</span></span> <a title="Permalink" class="permalink" href="#cha-spack">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Spack is a configurable Python-based package manager, automating
    the installation and fine-tuning of simulations and libraries.
    Spack can install many variants of the same build
    using different compilers, options, and MPI implementations.
    For more information, see the
    <a class="link" href="https://spack-tutorial.readthedocs.io/en/latest/" target="_blank">Spack Documentation</a>.
   </p></div></div></div></div><section class="sect1" id="spack-installation" data-id-title="Installing Spack"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Installing Spack</span></span> <a title="Permalink" class="permalink" href="#spack-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use this procedure to install Spack on any node in the cluster.
  </p><div class="procedure" id="pro-spack-installation" data-id-title="Installing and configuring Spack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.1: </span><span class="title-name">Installing and configuring Spack </span></span><a title="Permalink" class="permalink" href="#pro-spack-installation">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install Spack:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in spack</pre></div></li><li class="step"><p>
     Set up your environment with the appropriate script for your shell:
    </p><ul class="stepalternatives">
     <li class="step"><p>
       For bash/zsh/sh:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>. /usr/share/spack/setup-env.sh</pre></div></li>
     <li class="step"><p>
       For tcsh/csh:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>source /usr/share/spack/setup-env.csh</pre></div></li>
     <li class="step"><p>
       For fish:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>. /usr/share/spack/setup-env.fish</pre></div></li>
    </ul></li><li class="step"><p>
     It is recommended to install <code class="literal">bash-completion</code> so you can
     use <span class="keycap">TAB</span> key auto-completion for Spack commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in bash-completion</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack <span class="emphasis"><em>TAB</em></span>
activate      clone         dependencies  fetch         list          providers     solve         url
add           commands      dependents    find          load          pydoc         spec          verify
arch          compiler      deprecate     flake9        location      python        stage         versions
blame         compilers     dev-build     gc            log-parse     reindex       test          view
buildcache    concretize    develop       gpg           maintainers   remove        test-env
build-env     config        docs          graph         mark          repo          tutorial
cd            containerize  edit          help          mirror        resource      undevelop
checksum      create        env           info          module        restage       uninstall
ci            deactivate    extensions    install       patch         rm            unit-test
clean         debug         external      license       pkg           setup         unload</pre></div></li></ol></div></div></section><section class="sect1" id="spack-simple-example" data-id-title="Using Spack: simple example with netcdf-cxx4"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Using Spack: simple example with netcdf-cxx4</span></span> <a title="Permalink" class="permalink" href="#spack-simple-example">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This example procedure shows you different ways to build
  <code class="literal">netcdf-cxx4</code> with Spack.
 </p><div class="procedure" id="pro-spack-simple-example" data-id-title="Building netcdf-cxx4 with Spack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.2: </span><span class="title-name">Building <code class="literal">netcdf-cxx4</code> with Spack </span></span><a title="Permalink" class="permalink" href="#pro-spack-simple-example">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Show detailed information on <code class="literal">netcdf-cxx4</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack info netcdf-cxx4
AutotoolsPackage:   netcdf-cxx4

Description:
    NetCDF (network Common Data Form) is a set of software libraries and
    machine-independent data formats that support the creation, access, and
    sharing of array-oriented scientific data. This is the C++ distribution.

Homepage: https://www.unidata.ucar.edu/software/netcdf

Maintainers: @WardF

Tags:
    None

Preferred version: <span class="callout" id="spack-info-version">1</span>
    4.3.1    ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-cxx4-4.3.1.tar.gz

Safe versions:
    4.3.1    ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-cxx4-4.3.1.tar.gz
    4.3.0    ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-cxx4-4.3.0.tar.gz

Variants: <span class="callout" id="spack-info-variants">2</span>
    Name [Default]    Allowed values    Description
    ==============    ==============    ===================================================

    doxygen [on]      on, off           Enable doxygen docs
    pic [on]          on, off           Produce position-independent code (for shared libs)
    shared [on]       on, off           Enable shared library
    static [on]       on, off           Enable building static libraries

Installation Phases:
    autoreconf    configure    build    install

Build Dependencies: <span class="callout" id="spack-info-deps">3</span>
    autoconf  automake  doxygen  libtool  m4  netcdf-c

Link Dependencies:
    netcdf-c

Run Dependencies:
    None

Virtual Packages:
    None</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-info-version"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Spack uses the latest version by default, unless you specify otherwise
      with <code class="literal">@<em class="replaceable">VERSION</em></code>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-info-variants"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      You can disable the variant options using
      <code class="literal">-<em class="replaceable">VARIANT</em></code> or enable
      them using <code class="literal">+<em class="replaceable">VARIANT</em></code>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-info-deps"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The packages required by <code class="literal">netcdf-cxx4</code> must already be built.
     </p></td></tr></table></div></li><li class="step"><p>
    Build <code class="literal">netcdf-cxx4</code> with the variants <code class="literal">static</code>
    and <code class="literal">doxygen</code> disabled:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install netcdf-cxx4 -static -doxygen
==&gt; netcdf-c: Executing phase: 'autoreconf'
==&gt; netcdf-c: Executing phase: 'configure'
==&gt; netcdf-c: Executing phase: 'build'
==&gt; netcdf-c: Executing phase: 'install'
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-c-4.7.4-vry3tfp6kpq364gyxrj6fali4kqhirx7
==&gt; Installing netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq
==&gt; No binary for netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq found: installing from source
==&gt; Fetching https://spack-llnl-mirror.s3-us-west-2.amazonaws.com/_source-cache/archive/6a/6a1189a181eed043b5859e15d5c080c30d0e107406fbb212c8fb9814e90f3445.tar.gz
#################################################################################################################### 100.0%
==&gt; netcdf-cxx4: Executing phase: 'autoreconf'
==&gt; netcdf-cxx4: Executing phase: 'configure'
==&gt; netcdf-cxx4: Executing phase: 'build'
==&gt; netcdf-cxx4: Executing phase: 'install'
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq</pre></div></li><li class="step"><p>
    Rebuild <code class="literal">netcdf-cxx4</code> with the default variants, and
    specify the version:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install netcdf-cxx4@4.3.1
==&gt; Warning: Missing a source id for python@3.6.13
==&gt; Warning: Missing a source id for pkgconf@1.5.3
[+] /usr (external autoconf-2.69-tatq2aqbhboxbyjt2fsraoapgqwf3y5x)
[+] /usr (external automake-1.15.1-3d7wkh42v52c6n77t4p7l2i7nguryisl)
[+] /usr (external bison-3.0.4-y6ckc7e7mqnnkgmkbgcfbw5vgqzg5b6m)
[+] /usr (external cmake-3.17.0-jr4evnjsgd7uh5stt33woclti37743kg)
[+] /usr (external flex-2.6.4-vea2lhgajmeyjm6ei5d2bqvpss4ipors)
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libiconv-1.16-itovpc5jssshcgpeejrro6l7jn4ynaq7
[+] /usr (external python-3.6.13-rpf47wa6wfn7h3rnydpxijoczc6opno2)
[+] /usr (external libtool-2.4.6-ddch2qlie7t4ypbqg6kmf3uswqg2uylp)
[+] /usr (external m4-1.4.18-tloh56qj47ahddst5g2xqsawffuz5ew6)
[+] /usr (external pkgconf-1.5.3-gmxadsjg6q3xqwjwws5a4v4b4ugvi6p4)
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/util-macros-1.19.1-rpnlbst6v3oqjm7tfoxasmn7wlilpqut
[+] /usr (external xz-5.2.3-x3glm5yp2ixldbe7n557evglhygvlkqh)
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/zlib-1.2.11-z6y74kgd73yc23kr5252slbydmk4posh
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/doxygen-1.8.20-3griwieblqgb6ykc5avzkzrxmtaw4s2g
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/numactl-2.0.14-tcuvjjtkhnyf5ijrazenjra5h5dbj4in
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libpciaccess-0.16-y3w7dlktz22lmdj6fei4aj2f4t2rqu6l
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libxml2-2.9.10-d4c7cskvhn7qwzzb2wiq7rl67vbl44je
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/hwloc-1.11.11-rjdqchtk6i27lqxxwi4cvfyvrxxgwq7k
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/openmpi-3.1.6-ks5elgg25bbnzwa7fmv7lewbkrcp2qsx
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/hdf5-1.10.7-3cyidn4yvikyyuxehak7ftey2l57ku37
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-c-4.7.4-vry3tfp6kpq364gyxrj6fali4kqhirx7
==&gt; Installing netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy
==&gt; No binary for netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy found: installing from source
==&gt; Using cached archive: /var/spack/cache/_source-cache/archive/6a/6a1189a181eed043b5859e15d5c080c30d0e107406fbb212c8fb9814e90f3445.tar.gz
==&gt; netcdf-cxx4: Executing phase: 'autoreconf'
==&gt; netcdf-cxx4: Executing phase: 'configure'
==&gt; netcdf-cxx4: Executing phase: 'build'
==&gt; netcdf-cxx4: Executing phase: 'install'
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy</pre></div></li><li class="step"><p>
    Check which packages are now available with Spack:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack find
==&gt; 14 installed packages
-- linux-sle_hpc15-skylake / gcc@7.5.0 --------------------------
doxygen@1.8.20  hwloc@1.11.11  libpciaccess@0.16  netcdf-c@4.7.4     netcdf-cxx4@4.3.1  openmpi@3.1.6       xz@5.2.3
hdf5@1.10.7     libiconv@1.16  libxml2@2.9.10     netcdf-cxx4@4.3.1  numactl@2.0.14     util-macros@1.19.1  zlib@1.2.11</pre></div><p>
    In this example, there are now two versions of <code class="literal">netcdf-cxx4</code>.
    All of the build requirements for <code class="literal">netcdf-cxx4</code> are also present.
    If you want to show dependency hashes as well as versions, use the
    <code class="literal">-l</code> option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack find -l
==&gt; 14 installed packages
-- linux-sle_hpc15-skylake / gcc@7.5.0 --------------------------
3griwie doxygen@1.8.20  y3w7dlk libpciaccess@0.16  tiyqyxb netcdf-cxx4@4.3.1   x3glm5y xz@5.2.3
3cyidn4 hdf5@1.10.7     d4c7csk libxml2@2.9.10     tcuvjjt numactl@2.0.14      z6y74kg zlib@1.2.11
rjdqcht hwloc@1.11.11   vry3tfp netcdf-c@4.7.4     ks5elgg openmpi@3.1.6
itovpc5 libiconv@1.16   msiysdr netcdf-cxx4@4.3.1  rpnlbst util-macros@1.19.1</pre></div></li><li class="step"><p>
    Show the differences between the two versions of <code class="literal">netcdf-cxx4</code>:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Find the paths to the <code class="literal">netcdf-cxx4</code> packages:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack find --paths
==&gt; 15 installed packages
-- linux-sle_hpc15-skylake / gcc@7.5.0 --------------------------
doxygen@1.8.20      /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/doxygen-1.8.20-3griwieblqgb6ykc5avzkzrxmtaw4s2g
hdf5@1.10.7         /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/hdf5-1.10.7-3cyidn4yvikyyuxehak7ftey2l57ku37
hwloc@1.11.11       /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/hwloc-1.11.11-rjdqchtk6i27lqxxwi4cvfyvrxxgwq7k
hwloc@2.2.0         /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/hwloc-2.2.0-4lxxw65tzjeqhyxelowclnwqfb3m3rmk
libiconv@1.16       /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libiconv-1.16-itovpc5jssshcgpeejrro6l7jn4ynaq7
libpciaccess@0.16   /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libpciaccess-0.16-y3w7dlktz22lmdj6fei4aj2f4t2rqu6l
libxml2@2.9.10      /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/libxml2-2.9.10-d4c7cskvhn7qwzzb2wiq7rl67vbl44je
netcdf-c@4.7.4      /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-c-4.7.4-vry3tfp6kpq364gyxrj6fali4kqhirx7
netcdf-cxx4@4.3.1   /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq
netcdf-cxx4@4.3.1   /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy
numactl@2.0.14      /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/numactl-2.0.14-tcuvjjtkhnyf5ijrazenjra5h5dbj4in
openmpi@3.1.6       /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/openmpi-3.1.6-ks5elgg25bbnzwa7fmv7lewbkrcp2qsx
util-macros@1.19.1  /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/util-macros-1.19.1-rpnlbst6v3oqjm7tfoxasmn7wlilpqut
xz@5.2.3            /usr
zlib@1.2.11         /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/zlib-1.2.11-z6y74kgd73yc23kr5252slbydmk4posh</pre></div></li><li class="step"><p>
      Move into the parent directory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>cd /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0</pre></div></li><li class="step"><p>
      Run a <code class="command">diff</code> between each version's <code class="filename">spec.yaml</code>
      file. This file describes how the package was built.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>diff -ru netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq/.spack/spec.yaml netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy/.spack/spec.yaml
--- netcdf-cxx4-4.3.1-msiysdrdua3vv6izluhaeos4nyo5gslq/.spack/spec.yaml 2021-10-04 11:42:23.444000000 +0200
+++ netcdf-cxx4-4.3.1-tiyqyxb3eqpptrlcll6rlf27aisekluy/.spack/spec.yaml 2021-10-04 11:51:49.880000000 +0200
@@ -38,10 +38,10 @@
       version: 7.5.0
     namespace: builtin
     parameters:
-      doxygen: false
+      doxygen: true
       pic: true
       shared: true
-      static: false
+      static: true
       cflags: []
       cppflags: []
       cxxflags: []
@@ -54,8 +54,8 @@
         type:
         - build
         - link
-    hash: msiysdrdua3vv6izluhaeos4nyo5gslq
-    full_hash: oeylqzvergh3ckviaqyhy3idn7vyk3hi
+    hash: tiyqyxb3eqpptrlcll6rlf27aisekluy
+    full_hash: i6btamzuyoo343n63f3rqopkz7ymkapq
 - netcdf-c:
     version: 4.7.4
arch:</pre></div><p>
      This example shows that one version of <code class="literal">netcdf-cxx4</code> was
      built with <code class="literal">doxygen</code> and <code class="literal">static</code> enabled,
      and the other version of <code class="literal">netcdf-cxx4</code> was built with
      <code class="literal">doxygen</code> and <code class="literal">static</code> disabled.
     </p></li></ol></li></ol></div></div></section><section class="sect1" id="spack-complex-example" data-id-title="Using Spack: complex example with mpich"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.3 </span><span class="title-name">Using Spack: complex example with mpich</span></span> <a title="Permalink" class="permalink" href="#spack-complex-example">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This example procedure shows you different ways to build <code class="literal">mpich</code>
   with Spack.
  </p><div class="procedure" id="pro-spack-complex-example" data-id-title="Building mpich with Spack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.3: </span><span class="title-name">Building <code class="literal">mpich</code> with Spack </span></span><a title="Permalink" class="permalink" href="#pro-spack-complex-example">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     List the available versions of <code class="literal">mpich</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack versions mpich
==&gt; Safe versions (already checksummed):
  develop  3.3.2  3.3.1  3.3  3.2.1  3.2  3.1.4  3.1.3  3.1.2  3.1.1  3.1  3.0.4
==&gt; Remote versions (not yet checksummed):
==&gt; Warning: Found no unchecksummed versions for mpich</pre></div></li><li class="step"><p>
     Show detailed information on <code class="literal">mpich</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack info mpich
AutotoolsPackage:   mpich

Description:
    MPICH is a high performance and widely portable implementation of the
    Message Passing Interface (MPI) standard.

Homepage: http://www.mpich.org

Maintainers: @raffenet @yfguo

Tags:
    None

Preferred version:
    3.3.2      http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz

Safe versions:
    develop    [git] https://github.com/pmodels/mpich.git
    3.3.2      http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz
    3.3.1      http://www.mpich.org/static/downloads/3.3.1/mpich-3.3.1.tar.gz
    3.3        http://www.mpich.org/static/downloads/3.3/mpich-3.3.tar.gz
    3.2.1      http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz
    3.2        http://www.mpich.org/static/downloads/3.2/mpich-3.2.tar.gz
    3.1.4      http://www.mpich.org/static/downloads/3.1.4/mpich-3.1.4.tar.gz
    3.1.3      http://www.mpich.org/static/downloads/3.1.3/mpich-3.1.3.tar.gz
    3.1.2      http://www.mpich.org/static/downloads/3.1.2/mpich-3.1.2.tar.gz
    3.1.1      http://www.mpich.org/static/downloads/3.1.1/mpich-3.1.1.tar.gz
    3.1        http://www.mpich.org/static/downloads/3.1/mpich-3.1.tar.gz
    3.0.4      http://www.mpich.org/static/downloads/3.0.4/mpich-3.0.4.tar.gz

Variants:
    Name [Default]       Allowed values          Description
    =================    ====================    ========================================================================

    argobots [off]       on, off                 Enable Argobots support
    device [ch3]         ch3, ch4                Abstract Device Interface (ADI) implementation. The ch4 device is
                                                 currently in experimental state
    fortran [on]         on, off                 Enable Fortran support
    hwloc [on]           on, off                 Use external hwloc package
    hydra [on]<span class="callout" id="spack-boolean-variant">1</span>        on, off                 Build the hydra process manager
    libxml2 [on]         on, off                 Use libxml2 for XML support instead of the custom minimalistic
                                                 implementation
    netmod [tcp]<span class="callout" id="spack-nonboolean-variant">2</span>     tcp, mxm, ofi, ucx      Network module. Only single netmod builds are supported. For ch3 device
                                                 configurations, this presumes the ch3:nemesis communication channel.
                                                 ch3:sock is not supported by this spack package at this time.
    pci [on]             on, off                 Support analyzing devices on PCI bus
    pmi [pmi]            off, pmi, pmi2, pmix    PMI interface.
    romio [on]           on, off                 Enable ROMIO MPI I/O implementation
    slurm [off]          on, off                 Enable SLURM support
    verbs [off]          on, off                 Build support for OpenFabrics verbs.
    wrapperrpath [on]    on, off                 Enable wrapper rpath

Installation Phases:
    autoreconf    configure    build    install

Build Dependencies:
    argobots  automake   hwloc      libpciaccess  libxml2  pkgconfig  python  ucx
    autoconf  findutils  libfabric  libtool       m4       pmix       slurm

Link Dependencies:
    argobots  hwloc  libfabric  libpciaccess  libxml2  pmix  slurm  ucx

Run Dependencies:
    None

Virtual Packages:
    mpich@3: provides mpi@:3.0
    mpich@1: provides mpi@:1.3
    mpich provides mpi</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-boolean-variant"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p><code class="literal"><em class="replaceable">NAME</em> [on]</code> is a boolean variant.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-nonboolean-variant"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p><em class="replaceable">NAME=VALUE</em> is a non-boolean variant.</p></td></tr></table></div></li><li class="step"><p>
     Build <code class="literal">mpich</code> with some variants disabled:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install mpich@3.3.2 -romio -libxml2 -hydra -fortran
==&gt; mpich: Executing phase: 'autoreconf'
==&gt; mpich: Executing phase: 'configure'
==&gt; mpich: Executing phase: 'build'
==&gt; mpich: Executing phase: 'install'
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/mpich-3.3.2-3nexqqx2r4m7p2y7lmovuwvvobi2bygw</pre></div></li><li class="step"><p>
     Rebuild <code class="literal">mpich</code> with the default variants:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install mpich@3.3.2
.....
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/mpich-3.3.2-sahjm2uhsoc3bi2cljtypwuqaflhamnx</pre></div></li><li class="step"><p>
     Rebuild <code class="literal">mpich</code> with the compiler flag <code class="literal">cppflags</code>,
     a specific <code class="literal">libxml</code> version of <code class="literal">2.9.4</code>,
     an x86_64 <code class="literal">target</code>, and a non-boolean <code class="literal">netmod</code>
     option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install mpich@3.3.2<span class="callout" id="spack-comples-at">1</span> -static<span class="callout" id="spack-complex-variants">2</span> cppflags="-O3 -fPIC"<span class="callout" id="spack-complex-compilerflag">3</span> target=x86_64<span class="callout" id="spack-complex-target">4</span> ^libxml2@2.9.4<span class="callout" id="spack-complex-hat">5</span> netmod=tcp<span class="callout" id="spack-complex-name">6</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-comples-at"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">@</code> is an optional version specifier.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-complex-variants"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">+</code> or <code class="literal">-</code> specifies the boolean variant
       (on or off).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-complex-compilerflag"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">cflags, cxxflags, fflags, cppflags, ldflags</code>, and
       <code class="literal">ldlibs</code> are variants for the compiler flag.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-complex-target"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">target=<em class="replaceable">value</em></code> and
       <code class="literal">os=<em class="replaceable">value</em></code> specify the
       architecture and the operating system. You can list all of the available
       targets by running the command <code class="command">spack arch --known-targets</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-complex-hat"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">^</code> specifies a dependency.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-complex-name"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal"><em class="replaceable">name</em>=<em class="replaceable">value</em></code>
       specifies a non-boolean variant.
      </p></td></tr></table></div></li><li class="step"><p>
     List all of the available packages and their dependency hashes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack find -l
==&gt; 33 installed packages
-- linux-sle_hpc15-skylake / gcc@7.5.0 --------------------------
3griwie doxygen@1.8.20  y3w7dlk libpciaccess@0.16  vry3tfp netcdf-c@4.7.4      x3glm5y xz@5.2.3
3cyidn4 hdf5@1.10.7     to2atk6 libpciaccess@0.16  msiysdr netcdf-cxx4@4.3.1   bzeebyp xz@5.2.3
rjdqcht hwloc@1.11.11   4cyoxau libxml2@2.9.4      tiyqyxb netcdf-cxx4@4.3.1   z6y74kg zlib@1.2.11
4lxxw65 hwloc@2.2.0     d4c7csk libxml2@2.9.10     tcuvjjt numactl@2.0.14      m4zmub6 zlib@1.2.11
yof3lps hwloc@2.2.0     3nexqqx mpich@3.3.2        ks5elgg openmpi@3.1.6
itovpc5 libiconv@1.16   sahjm2u mpich@3.3.2        rpnlbst util-macros@1.19.1
5mvdlgl libiconv@1.16   jxbspwk mpich@3.3.2        boclx6a util-macros@1.19.1

-- linux-sle_hpc15-x86_64 / gcc@7.5.0 ---------------------------
yzxibyd hwloc@2.2.0    7wmqik2 libpciaccess@0.16  6xjiutf mpich@3.3.2         zmfrpzf xz@5.2.3
mm7at5h libiconv@1.16  jrtvvdj libxml2@2.9.4      jjxbukq util-macros@1.19.1  jxqtsne zlib@1.2.11</pre></div></li><li class="step"><p>
     Show the specifications and output dependencies of
     <code class="literal">/6xjiutf</code> (<code class="literal">mpich</code>):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack find --deps /6xjiutf
==&gt; 1 installed package
-- linux-sle_hpc15-x86_64 / gcc@7.5.0 ---------------------------
mpich@3.3.2
    hwloc@2.2.0
        libpciaccess@0.16
        libxml2@2.9.4
            libiconv@1.16
            xz@5.2.3
            zlib@1.2.11</pre></div></li></ol></div></div></section><section class="sect1" id="spack-compiler" data-id-title="Using a specific compiler"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.4 </span><span class="title-name">Using a specific compiler</span></span> <a title="Permalink" class="permalink" href="#spack-compiler">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In this example procedure, the goal is to build <code class="literal">mpich</code>
   with <code class="literal">gcc-10.2.0</code>.
  </p><div class="procedure" id="pro-spack-compiler" data-id-title="Using a specific compiler with Spack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.4: </span><span class="title-name">Using a specific compiler with Spack </span></span><a title="Permalink" class="permalink" href="#pro-spack-compiler">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/spack.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install <code class="literal">gcc-10.2.0</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install gcc@10.2.0
.....
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-7.5.0/gcc-10.2.0-tkcq6d6xnyfgyqsnpzq36dlk2etylgp7</pre></div></li><li class="step"><p>
     Test whether <code class="literal">gcc</code> is usable by loading it and attempting
     to build <code class="literal">mpich</code>, using the <code class="literal">%</code>
     option to specify that you want to use <code class="literal">gcc-10.2.0</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>module load gcc-10.2.0-gcc-7.5.0-tkcq6d6 <span class="callout" id="spack-module-load">1</span>
<code class="prompt root"># </code>gcc --version
gcc (Spack GCC) 10.2.0
<code class="prompt root"># </code>spack install mpich@3.2.1 %gcc@10.2.0
==&gt; Error: No compilers with spec gcc@10.2.0 found for operating system sle_hpc15 and target skylake.
Run 'spack compiler find' to add compilers or 'spack compilers' to see which compilers are already recognized by spack.</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#spack-module-load"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="command">module load gcc-10.2.0</code> also works if you have only
       one version of <code class="literal">gcc-10.2.0</code> available in Spack.
      </p></td></tr></table></div><p>
     In this example, the compiler cannot be found yet.
    </p></li><li class="step"><p>
     Update the list of available compilers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack compiler find
==&gt; Added 1 new compiler to /home/tux/.spack/linux/compilers.yaml
    gcc@10.2.0
==&gt; Compilers are defined in the following files:
    /home/tux/.spack/linux/compilers.yaml</pre></div></li><li class="step"><p>
     Rerun the command to build <code class="literal">mpich</code> with <code class="literal">gcc-10.2.0</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>spack install mpich@3.2.1 %gcc@10.2.0
.....
[+] /usr/opt/spack/linux-sle_hpc15-skylake/gcc-10.2.0/mpich-3.2.1-e56rcwtqofh2xytep5pjgq6wrxwmsy25</pre></div></li></ol></div></div></section></section><section xml:lang="en" class="chapter" id="cha-dolly" data-id-title="Dolly clone tool"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Dolly clone tool</span></span> <a title="Permalink" class="permalink" href="#cha-dolly">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Dolly is used to send data from a management server to many other nodes.
    It can distribute files, container images,
    partitions, or whole storage devices.
   </p></div></div></div></div><section class="sect1" id="dolly-cloning-process" data-id-title="Dolly cloning process"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Dolly cloning process</span></span> <a title="Permalink" class="permalink" href="#dolly-cloning-process">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   One machine is the management server and distributes the data to the nodes.
   The management server stores the image, partition, disk, or data to be cloned.
   This machine runs dolly as a server. All other nodes are dolly clients. They
   receive the data from the ring, store it locally, and send it to the next node
   in the ring. All of this happens at the same time, so transferring data to
   one node or to hundreds of nodes takes the same amount of time.
  </p><p>
   Dolly creates a virtual TCP ring to distribute data.
  </p><div class="figure" id="fig-dolly-cloning-process"><div class="figure-contents"><div class="mediaobject"><a href="images/dolly_cloning_process.png"><img src="images/dolly_cloning_process.png" width="90%" alt="Dolly cloning process" title="Dolly cloning process"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 9.1: </span><span class="title-name">Dolly cloning process </span></span><a title="Permalink" class="permalink" href="#fig-dolly-cloning-process">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="using-dolly" data-id-title="Using dolly"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Using dolly </span></span> <a title="Permalink" class="permalink" href="#using-dolly">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Install dolly on the management server and all dolly client nodes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in dolly</pre></div><div id="id-1.11.4.4" data-id-title="Automatically opened ports" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Automatically opened ports</div><p>
  Installing the <span class="package">dolly</span> package automatically opens the TCP
  ports 9997 and 9998.
 </p></div><p>
   The <code class="command">dolly</code> command requires the following information,
   either directly on the command line or from a configuration file:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Data to send over the network from the dolly server. This could be a
     storage device, a file (<code class="filename">gzip</code> or other file types),
     or an image (containers or other data).
    </p></li><li class="listitem"><p>
     A target on the dolly clients. The target must be the same data type as
     the input from the dolly server. For example, dolly cannot send a
     <code class="filename">gzip</code> file to a storage device.
    </p></li><li class="listitem"><p>
     A list of the dolly client nodes where you want to send the data.
    </p></li></ul></div><p>
   Any other parameters are optional. For more information, see
   <code class="command">man dolly</code> and <a class="xref" href="#dolly-support" title="9.4. Dolly limitations">Section 9.4, “Dolly limitations”</a>.
  </p><div class="procedure" id="pro-using-dolly" data-id-title="Cloning data from a dolly server to dolly clients"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 9.1: </span><span class="title-name">Cloning data from a dolly server to dolly clients </span></span><a title="Permalink" class="permalink" href="#pro-using-dolly">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the dolly server, run the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dolly -s<span class="callout" id="dolly-server-parameter">1</span> -v<span class="callout" id="dolly-server-verbose">2</span> -o <em class="replaceable">LOGFILE</em><span class="callout" id="dolly-server-log">3</span> -I <em class="replaceable">INPUT</em><span class="callout" id="dolly-server-input">4</span> -O <em class="replaceable">OUTPUT</em><span class="callout" id="dolly-server-output">5</span> -H <em class="replaceable">NODE1,NODE2,..</em><span class="callout" id="dolly-server-nodes">6</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-parameter"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Specifies that this node is the dolly server (the node that sends the data).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-verbose"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Switches dolly to verbose mode, which is helpful for debugging.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-log"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The file that statistical information is written to.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-input"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The data to clone.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-output"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The target that will store the data on each dolly client.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-server-nodes"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A comma-separated list of dolly clients to receive the data.
      </p></td></tr></table></div><p>
     For example, the following command sends <code class="literal">/dev/sdc1</code> to
     the nodes <code class="literal">sle152</code>, <code class="literal">sle153</code>,
     <code class="literal">sle154</code>, and <code class="literal">sle155</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dolly -s -v -o /tmp/dolly.log -I /dev/sdc1 -O /dev/sdc1 -H sle152,sle153,sle154,sle155</pre></div></li><li class="step"><p>
     On each dolly client, start <code class="command">dolly</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dolly -v</pre></div><p>
     You can run this command on multiple nodes at once using <code class="command">pdsh</code>.
     See <a class="xref" href="#sec-remote-pdsh" title="3.2. pdsh — parallel remote shell program">Section 3.2, “pdsh — parallel remote shell program”</a>.
    </p></li></ol></div></div><div class="complex-example"><div class="example" id="ex-dolly-server-verbose-output" data-id-title="Dolly server verbose output"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.1: </span><span class="title-name">Dolly server verbose output </span></span><a title="Permalink" class="permalink" href="#ex-dolly-server-verbose-output">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    This example shows typical verbose output from the <code class="command">dolly</code>
    command on the dolly server:
   </p><div class="verbatim-wrap"><pre class="screen">'writing '192.168.255.2'
'writing '192.168.255.3'
'writing '192.168.255.4'
'writing '192.168.255.5'
'Parameter file:
infile = '/dev/sdc1'
outfile = '/dev/sdc1'
using data port 9998
using ctrl port 9997
myhostname = 'sle151'
fanout = 1
nr_childs = 1
server = 'sle151'
I'm the server.
I'm not the last host.
There are 4 hosts in the ring (excluding server):
        '192.168.255.2'
        '192.168.255.3'
        '192.168.255.4'
        '192.168.255.5'
Next hosts in ring:
        192.168.255.2 (0)
All parameters read successfully.
No compression used.
Using transfer size 4096 bytes.

Trying to build ring...
Connecting to host 192.168.255.2... Send buffer 0 is 131072 bytes
data control.
Waiting for ring to build...
Host got parameters '192.168.255.2'.
Machines left to wait for: 4
Host ready '192.168.255.2'.
Machines left to wait for: 3
Host got parameters '192.168.255.3'.
Machines left to wait for: 3
Host ready '192.168.255.3'.
Machines left to wait for: 2
Host got parameters '192.168.255.4'.
Machines left to wait for: 2
Host ready '192.168.255.4'.
Machines left to wait for: 1
Host got parameters '192.168.255.5'.
Machines left to wait for: 1
Host ready '192.168.255.5'.
Machines left to wait for: 0
Accepted.
Server: Sending data...
Sent MB: 15854, MB/s: 29.655, Current MB/s: 111.111
Read 15854469120 bytes from file(s).
Writing maxbytes = 15854469120 to ctrlout
Sent MB: 15854.
Synced.
Waiting for child 0.
Clients done.
Time: 534.627532
MBytes/s: 29.655
Aggregate MBytes/s: 118.621
Transmitted.</pre></div></div></div></div><div class="complex-example"><div class="example" id="dolly-client-verbose-output" data-id-title="Dolly client verbose output"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.2: </span><span class="title-name">Dolly client verbose output </span></span><a title="Permalink" class="permalink" href="#dolly-client-verbose-output">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    This example shows typical verbose output from the <code class="command">dolly</code>
    command on the dolly clients:
   </p><div class="verbatim-wrap"><pre class="screen">Trying to build ring...
Buffer size: 98304
Receive buffer is 196608 bytes
Accepting...control...
Trying to read parameters...done.
Parsing parameters...
done.
192.168.255.2 is number 0
Parameter file:
infile = '/dev/sdc1'
outfile = '/dev/sdc1'
using data port 9998
using ctrl port 9997
myhostname = '192.168.255.2'
fanout = 1
nr_childs = 1
server = 'sle151'
I'm not the server.
I'm not the last host.
There are 4 hosts in the ring (excluding server):
        '192.168.255.2'
        '192.168.255.3'
        '192.168.255.4'
        '192.168.255.5'
Next hosts in ring:
192.168.255.3 (1)
All parameters read successfully.
No compression used.
Using transfer size 4096 bytes.
Connected data...done.
Connecting to host 192.168.255.3...
data control.
Accepted.
Receiving...
Transfered MB: 15854, MB/s: 29.655, Current MB/s: 116.661
Max. bytes will be 15854469120 bytes. 49152 bytes left.
Transfered MB: 15854, MB/s: 29.655

Synced.
Transmitted.</pre></div></div></div></div></section><section class="sect1" id="dolly-configuration-file" data-id-title="Dolly configuration file"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.3 </span><span class="title-name">Dolly configuration file</span></span> <a title="Permalink" class="permalink" href="#dolly-configuration-file">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can use a dolly configuration file with the <code class="option">-f</code> parameter instead of providing the information manually on the command line.
   The following example shows a typical configuration file called <code class="filename">/etc/dolly.cfg</code>:
  </p><div class="verbatim-wrap"><pre class="screen">infile /tmp/sle15.sif<span class="callout" id="dolly-config-in">1</span>
outfile /data/sle15.sif<span class="callout" id="dolly-config-out">2</span>
server sle151<span class="callout" id="dolly-config-server">3</span>
firstclient sle152<span class="callout" id="dolly-config-first">4</span>
lastclient sle154<span class="callout" id="dolly-config-last">5</span>
clients 3<span class="callout" id="dolly-config-nbclients">6</span>
sle152<span class="callout" id="dolly-config-nodes">7</span>
sle153
sle154
endconfig<span class="callout" id="dolly-config-end">8</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-in"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The data to send over the network from the dolly server.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-out"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The file or device that will store the data on the dolly clients.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-server"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The name of the dolly server.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-first"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The first dolly client in the ring.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-last"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The last dolly client in the ring.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-nbclients"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Specifies how many dolly clients are in the ring.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-nodes"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The list of dolly clients, one per line.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#dolly-config-end"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Specifies the end of the configuration file.
    </p></td></tr></table></div><p>
   To use this configuration file, run the following command on the dolly server:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dolly -v -s -f /etc/dolly.cfg</pre></div></section><section class="sect1" id="dolly-support" data-id-title="Dolly limitations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.4 </span><span class="title-name">Dolly limitations</span></span> <a title="Permalink" class="permalink" href="#dolly-support">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/dolly.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Be aware of the following restrictions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The output data type must be the same as the input data type. Mixing the
     type of input and output can lead to data corruption.
    </p></li><li class="listitem"><p>
     Only clone partitions that are identical in size on the dolly server and
     the client node.
    </p></li><li class="listitem"><p>
     Only clone strictly identical storage devices, or corruption can occur.
    </p></li></ul></div><p>
   The following command line parameters are not supported and are provided as a
   technology preview only:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="option">-S</code>: Ignoring the FQDN is not supported.
    </p></li><li class="listitem"><p>
     <code class="option">-6</code>: Using IPv6 is not supported.
    </p></li><li class="listitem"><p>
     <code class="option">-n</code>: Not doing a sync before exiting is not supported as
     this can lead to data corruption.
    </p></li><li class="listitem"><p>
     <code class="option">-c</code>: Specifying the uncompressed size of a compressed file
     should only be used for performance statistics.
    </p></li></ul></div><p>
   The following configuration file options are not supported and are provided as
   a technology preview only:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="option">compressed</code>: Using the compression option is not supported.
    </p></li><li class="listitem"><p>
     <code class="option">split</code>: Splitting files is not supported (infile or outfile).
    </p></li><li class="listitem"><p>
     <code class="option">fanout</code>: This option must be set to 1 (a linear list).
     A binary tree or more is not supported.
    </p></li><li class="listitem"><p>
     <code class="option">segsize</code>: This benchmark switch is not supported.
    </p></li><li class="listitem"><p>
     <code class="option">add</code>: Using more than one interface to clone data is not supported.
    </p></li></ul></div></section></section><div class="legal-section"><section class="appendix" id="id-1.12" data-id-title="GNU licenses"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">GNU licenses</span></span> <a title="Permalink" class="permalink" href="#id-1.12">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_legal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This appendix contains the GNU Free Documentation License version 1.2.
 </p><section class="sect1" id="id-1.12.4" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title legal"><span class="title-number-name"><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.12.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-hpc/edit/main/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="https://www.gnu.org/copyleft/" target="_blank">https://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.12.4.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.12.4.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>